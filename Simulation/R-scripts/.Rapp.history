warnings()
gR <- randomDAG(10, prob=.2)#
nummods <- 8#
graphlist <- list(gR)#
#igR <- igraph.from.graphNEL(gR)#
#l1 <- layout.fruchterman.reingold(igR)#
#
for (i in 1:nummods){#
	whichmod <- sample(4, size=1)#
	if (whichmod==1){#
		cat("removing a node\n")#
		graphlist[[i+1]] <- modRemoveNode(graphlist[[i]])#
	} else if (whichmod==2){#
		cat("removing an edge\n")#
		graphlist[[i+1]] <- modRemoveEdge(graphlist[[i]])#
	} else if (whichmod==3){#
		cat("duplicating a node\n")#
		graphlist[[i+1]] <- modDupNode(graphlist[[i]])#
	} else if (whichmod==4){#
		cat("adding an edge\n")#
		graphlist[[i+1]] <- modAddEdge(graphlist[[i]])#
	}#
}#
par(mfrow=c(3,3))#
for (i in 1:(nummods+1)) plot(graphlist[[i]])
ls()
setwd("/Users/lizzie/Box Sync/Probabilistic-Graphical-Models/Homework/hw3/wiki_datsets")#
#
###############################################
# pre-process dat#
docs <- readLines("wiki_docs.txt")#
docs <- strsplit(docs, split=" ")#
for(i in 1:length(docs)) docs[[i]] <- as.numeric(docs[[i]])#
#
dat <- docs#
#
vocab <- read.table("wiki_vocab.txt", sep="\t")#
names(vocab) <- c("word", "id")#
vocab[,1] <- as.character(vocab[,1])#
vocab[,2] <- as.numeric(as.character(vocab[,2]))#
#
###############################################
# initialize beta using the dat#
initbeta <- function(dat, vocab, smooth=1){#
	dat <- unlist(dat)#
	dat <- table(dat)#
	tab <- rep(0, nrow(vocab))#
	names(tab) <- vocab$id#
	tab[names(dat)] <- dat#
	tab <- tab + smooth#
	beta <- matrix(rep(tab/sum(tab), k), ncol=k)#
	return(beta)#
}
setwd("/Users/lizzie/Box Sync/Probabilistic-Graphical-Models/Homework/hw3/wiki_datsets")
pc
library(pcalg)
pc
?match.call
?match.arg
skeleton
?rfci
showClass("fciAlgo")
7776^5
7776^5/100000000000
(7776^5/100000000000)/(60*60)
(7776^5/100000000000)/(60*60*24)
(7776^5/100000000000)/(60*60*24*365)
(26^23/100000000000)/(60*60*24*365)
209 / 209 + (50000 - 1934)
209 / (209 + (50000 - 1934))
*100
209 *100 / (209 + (50000 - 1934))
209 *100 / (209 + (50000*49999 - 1934))
ls()
learnnetworkPC
learnnetworkPC <-function(genes,paths,data,defaultalpha) {#
	genes<-setdiff(genes,setdiff(genes,colnames(data)))#
	if(is.null(paths)){#
		paths<-matrix(0,nrow=length(genes),ncol=length(genes))#
		colnames(paths)<-genes#
		rownames(paths)<-genes#
	}#
	levels<-data[,genes]#
	cSums<-colSums(levels)#
	levels<-levels[,-which(cSums==0)]#
	genes<-genes[-which(cSums==0)]#
	S=cor(levels)#
	n<-length(genes)#
	N<-dim(levels)[1]#
	b<-(.5-defaultalpha)#
	util<-paths>0#
	class(util)<-"numeric"#
	util<-(paths+1)-util#
	alpha<-.5 - (b/util)	#
	G.PC=pc(suffStat=list(C=S,n=dim(data)[1]),indepTest=gaussCItest, alpha=alpha,labels=genes,conservative=TRUE,m.max=3,verbose=FALSE)#
	G.PC<-make.moral(G.PC@graph)#
	return(G.PC)#
}
startTime<-system.Time()#
G.PC <- learnnetworkPC(allgenes[,1],allsharepaths, controldata, .05)#
endTime<-system.Time()
system.time()
library(pcalg)
time.now()
now()
library(pcalg)
.libPaths()
library(pcalg)
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='lizziesilver', token='4718CA3F9CF3972D29E0D005677698DF', secret='sjJqU9Ydx3Jp3dsVYyZ/gfE5XYwSmdcNe+8rFW0N')
library(shinyapps)#
shinyapps::deployApp('path/to/your/app')
?update.packages
install.packages('RODBC')
609.92*2 + 549.08*2 + 492.31 - 3295
library(pcal)
library(pcalg)
library(graph)
source("http://bioconductor.org/biocLite.R")#
biocLite("graph")
library(pcalg)
?pc
?ges
install.packages(pcalg)
install.packages('pcalg')
install.packages('CAM')
bob = 1:412
bob
412/3
createFolds(bob, k = 3, list = TRUE, returnTrain = FALSE)
library(caret)
createFolds(bob, k = 3, list = TRUE, returnTrain = FALSE)
joe = createFolds(bob, k = 3, list = TRUE, returnTrain = FALSE)
joe[[1]]
write.table(joe[[1]], file="~/dssg/lizziesjob.txt", sep="\n")
write.table(joe[[1]], file="~/dssg/lizziesjob.txt", sep="")
write.table(joe[[1]], file="~/dssg/lizziesjob.txt", sep="", header=FALSE, rownums=FALSE)
?write.table
write.table(joe[[1]], file="~/dssg/lizziesjob.txt", sep="", header=FALSE, row.names=FALSE)
write.table(joe[[1]], file="~/dssg/lizziesjob.txt", sep="", col.names=FALSE, row.names=FALSE)
sam = vector(length=length(bob))
sam[joe[[1]]] = "Lizzie"
sam[joe[[2]]] = "Tom"
sam[joe[[3]]] = "Amy"
sam
write.table(sam, file="~/dssg/emailpartition.txt", sep="", col.names=FALSE, row.names=FALSE, quote=FALSE)
sam = vector(length= 412)#
#
bob = 1:252#
createFolds(bob, k = 3, list = TRUE, returnTrain = FALSE)#
sam[joe[[1]]] = "Lizzie"#
sam[joe[[2]]] = "Tom"#
sam[joe[[3]]] = "Amy"#
#
bob = 253:412#
createFolds(bob, k = 3, list = TRUE, returnTrain = FALSE)#
sam[joe[[1]]] = "Lizzie"#
sam[joe[[2]]] = "Tom"#
sam[joe[[3]]] = "Amy"#
#
write.table(sam, file="~/dssg/emailpartition.txt", sep="", col.names=FALSE, row.names=FALSE, quote=FALSE)
2551.82-505.43
2551.82-505.43 - 600
2551.82-505.43 - 600 - 3295
650 + 515 + 515 + 625
650 + 515 + 515 + 625 + 2150
3295 - (519.08 +579.92)
519.08 + 579.92
3295 - 1099
?solve.qr
?match.call
library(rJava)
find.packages(rJava)
find.package(rJava)
library(rJava)
detach("package:rJava", unload=TRUE)
Sys.getenv('LD_LIBRARY_PATH')
install.packages('rJava', type='source')
library(rJava)
install.packages(rJava)
install.packages("rJava")
library(rJava)
remove.packages("rJava")
install.packages("rJava")
library(rJava)
remove.packages("rJava")
find.package("rJava")
library(rJava)
.jinit)
.jinit())
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
library(rJava)
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
Sys.getenv("DYLD_FALLBACK_LIBRARY_PATH")
detach("package:rJava", unload=TRUE)
Sys.setenv(DYLD_FALLBACK_LIBRARY_PATH="/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home/jre/lib/server/")
library(rJava)#
.jinit()#
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
.jinit("/Users/lizzie/Dissertation/Tetrad-jars/tetrad-5.2.1-3.jar") # this starts the JVM
Sys.setenv(DYLD_FALLBACK_LIBRARY_PATH="/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home/jre/lib/server/")
library(rJava)
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
system("java -version")
Sys.getenv("LD_LIBRARY_PATH")
Sys.getenv("R_HOME")
Sys.setenv(JAVA_HOME='/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk')
detach("package:rJava", unload=TRUE)
Sys.setenv(JAVA_HOME='/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk')
Sys.setenv(DYLD_FALLBACK_LIBRARY_PATH="/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home/jre/lib/server/")
library(rJava)
.jinit()
DYLD_PRINT_LIBRARIES=1
DYLD_PRINT_LIBRARIES
library(rJava)
detach("package:rJava", unload=TRUE)
library(rJava, DYLD_PRINT_LIBRARIES=1)
?library
getOption("verbose")
library(rJava, verbose=TRUE)
detach("package:rJava", unload=TRUE)
install.packages("rJava",,"http://rforge.net/",type="source")
library(rJava)
find.package("rJava")
ls()
# load required libraries#
library(graph)#
library(RBGL)#
library(rJava)#
library(pcalg)#
library(stringr)#
# library(gRbase)#
# library(dplyr)#
#
# load configuration file#
setwd("/Users/lizzie/Dissertation_code/Simulation/R-scripts")#
source("simulation_config.R")#
source("graph-rewriting-grammar.R")
if (file.exists(last_id_log_file)){#
	last_run_ids = read.table(last_id_log_file, header=TRUE, sep=",")#
} else {#
	last_run_ids = data.frame("run_id"=0, "progenitor_graph_id"=0, #
							  "evolved_graph_id"=0, "dataset_id"=0, #
 							  "pooled_dataset_id"=0, "search_id"=0)#
}#
new_run_ids = last_run_ids + 1#
#
#################################################################################
# file to log every run of the pipeline#
# should contain: run id, date, progenitor graph id, #
#     list of evolved graph ids, list of dataset ids, list of pooled data ids, #
#     list of search ids, list of evaluation set ids, start time, end time,#
#     duration#
sim_run_log = data.frame("run_id"=new_run_ids$run_id, #
						 "start_time"=Sys.time(),#
						 "end_time"=NA,#
						 "duration"=NA,#
						 "progenitor_graph_id"=new_run_ids$progenitor_graph_id,#
						 "evolved_graph_ids"=NA,#
						 "dataset_ids"=NA,#
						 "pooled_data_id"=new_run_ids$pooled_dataset_id,#
						 "search_ids"=NA,#
						 "evaluation_ids"=NA)#
#
#################################################################################
# Graph generation:#
# 	Generate small number of random graphs, the progenitors. #
# 		Choices: number of nodes per graph, number of edges per graph (or #
#           connection probability)#
# 		Log: features of graphs, plus a unique id, plus params of generation #
#           process#
# 		Save graph object#
pgid = new_run_ids$progenitor_graph_id#
progenitor <- randomDAG(num_nodes, prob_edges)#
#
# save and log aspects of the graph in a CSV:#
progenitor_graph_log = data.frame("progenitor_graph_id"=pgid,#
								  "sim_run_id"=new_run_ids$run_id,#
								  "num_nodes"=num_nodes,#
								  "prob_edges"=prob_edges,#
								  "num_edges"=length(unlist(edges(progenitor))))#
#
write.table(progenitor_graph_log, progenitor_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(progenitor_graph_log_file), #
    		col.names = "!"(file.exists(progenitor_graph_log_file)))#
#
# save and log an ASCII representation of the graph, and an R object:#
ascii_filename = paste(progenitor_graph_log_dir, "ASCII/progenitor_graph_",#
					   pgid, ".txt", sep="")#
r_filename = paste(progenitor_graph_log_dir, "R_objects/progenitor_graph_",#
				   pgid, ".R", sep="")#
save(progenitor, file=r_filename)#
dput(progenitor, file=ascii_filename)
evolved_list = list()#
for (i in 1:num_evolved){#
	evolved_list[[i]] <- evolveDAG(num_mods, progenitor)#
}#
#
# save and log evolved graphs here#
first_ev_id = new_run_ids$evolved_graph_id#
last_ev_id = new_run_ids$evolved_graph_id + num_evolved - 1#
evolved_graph_ids = seq(first_ev_id, last_ev_id)#
#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
#
num_nodes=c()#
num_edges=c()#
mods = c()#
for (i in 1:num_evolved){#
	g = evolved_list[[i]][[1]]#
	num_nodes=c(num_nodes, numNodes(g))#
	num_edges=c(num_edges, length(unlist(edges(g))))#
	mods_i = paste(evolved_list[[i]][[2]], collapse="; ")#
	mods = c(mods, mods_i)#
	ascii_filename = paste(evolved_graph_log_dir, "ASCII/evolved_graph_",#
						   evolved_graph_ids[i], ".txt", sep="")#
	r_filename = paste(evolved_graph_log_dir, "R_objects/evolved_graph_",#
					   evolved_graph_ids[i], ".R", sep="")#
	save(g, file=r_filename)#
	dput(g, file=ascii_filename)#
}#
#
# todo: add params of evolution process, once those are not hard-coded#
evolved_graph_log = data.frame("evolved_graph_ids" = evolved_graph_ids,#
							"progenitor_id" = rep(pgid, num_evolved),#
							"sim_run_id" = rep(new_run_ids$run_id, num_evolved),#
							"num_nodes"= num_nodes,#
							"num_edges"= num_edges,#
							"num_mods" = num_mods,#
							"mods"= mods)#
write.table(evolved_graph_log, evolved_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(evolved_graph_log_file), #
    		col.names = "!"(file.exists(evolved_graph_log_file)))#
#
sim_run_log$evolved_graph_ids = paste(evolved_graph_ids, collapse=";")#
#
#################################################################################
# 	Generate data from each descendant graph. #
# 		Choices: parameterization, number of data points to generate.#
# 		Log: data filename, number of rows and columns, params of data #
#           generation, id of generator graph#
# 		Save data#
# 		At end of each data generation: remove graph from memory#
#
data_list = list()#
#
dataset_ids = evolved_graph_ids#
#
for (i in 1:num_evolved){#
	if (parameterization=="linear_gaussian"){#
		evolved_list[[i]][[1]] <- dag2gausspardag(evolved_list[[i]][[1]])#
	} else {#
		cat("error: parameterization not listed or not recognized\n")#
		break#
	}#
	data_list[[i]] <- rmvnorm.ivent(sample_size, evolved_list[[i]][[1]])#
	# save dataset#
	data_filename = paste(dataset_log_dir, "dataset_", dataset_ids[i], ".csv", #
						  sep="")#
	write.table(data_list[[i]], data_filename, sep=",", row.names=FALSE, #
				col.names=TRUE)#
}#
# log features of dataset#
# todo: add params of data generation#
# todo: log parameterized graph!!!#
dataset_log = data.frame("dataset_id" = dataset_ids,#
						 "evolved_graph_id"=evolved_graph_ids,#
						 "parameterization" = parameterization,#
						 "num_rows"=rep(sample_size, num_evolved),#
						 "num_columns"= num_nodes)#
#
write.table(dataset_log, dataset_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(dataset_log_file), #
    		col.names = "!"(file.exists(dataset_log_file)))#
#
sim_run_log$dataset_ids = paste(dataset_ids, collapse=";")#
#
# # if not doing evaluation in same loop, get rid of graphs:#
# rm(evolved_list) #
#
# todo: Getting some errors in this step - need to add some unit tests for the graph#
# modification procedure.#
# does duplicating a node cause problems? Maybe if you add an edge from the #
# duplicated node to other nodes? Need to check this.#
#
#################################################################################
# Data pooling:#
#
# 	Pool data from graphs descended from the same progenitor. #
# 		Choices: amount of data from each graph to include; number of graphs #
#           to include#
# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row #
#           numbers from the source data that were used in pooling, #
#
vars = colnames(data_list[[1]])#
rows_included_list = list()#
for (i in 1: num_evolved){#
	vars = intersect(vars, colnames(data_list[[i]]))#
	rows_included = sample(1:sample_size, pooled_sample_sizes[i])#
	rows_included_list[[i]] <- 1:sample_size %in% rows_included#
}#
# save and log the rows included and the vars included #
pool_data_list <- list()#
for (i in 1: num_evolved){#
	pool_data_list[[i]] <- subset(data_list[[i]], #
								  subset=rows_included_list[[i]], select=vars)#
}#
pool_data_frame = do.call(rbind, pool_data_list)#
#
pdid = new_run_ids$pooled_dataset_id#
#
pool_log = data.frame("pooled_dataset_id" = pdid,#
					  "component_dataset_ids" = paste(dataset_ids, collapse=";"),#
					  "pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))#
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")#
dput(rows_included, file=pool_filename)#
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(pooled_data_log_file), #
    		col.names = "!"(file.exists(pooled_data_log_file)))
dput(rows_included, file=pool_filename)
dput(rows_included_list, file=pool_filename)
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])
target_small_data
# load configuration file#
setwd("/Users/lizzie/Dissertation_code/Simulation/R-scripts")#
source("simulation_config.R")#
source("graph-rewriting-grammar.R")#
#
#################################################################################
# Load unique ids of last run, if this is not the first run:#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
if (file.exists(last_id_log_file)){#
	last_run_ids = read.table(last_id_log_file, header=TRUE, sep=",")#
} else {#
	last_run_ids = data.frame("run_id"=0, "progenitor_graph_id"=0, #
							  "evolved_graph_id"=0, "dataset_id"=0, #
 							  "pooled_dataset_id"=0, "search_id"=0)#
}#
new_run_ids = last_run_ids + 1#
#
#################################################################################
# file to log every run of the pipeline#
# should contain: run id, date, progenitor graph id, #
#     list of evolved graph ids, list of dataset ids, list of pooled data ids, #
#     list of search ids, list of evaluation set ids, start time, end time,#
#     duration#
sim_run_log = data.frame("run_id"=new_run_ids$run_id, #
						 "start_time"=Sys.time(),#
						 "end_time"=NA,#
						 "duration"=NA,#
						 "progenitor_graph_id"=new_run_ids$progenitor_graph_id,#
						 "evolved_graph_ids"=NA,#
						 "dataset_ids"=NA,#
						 "pooled_data_id"=new_run_ids$pooled_dataset_id,#
						 "target_graph_id"=new_run_ids$evolved_graph_id,#
						 "search_ids"=NA,#
						 "evaluation_ids"=NA)#
#
#################################################################################
# Graph generation:#
# 	Generate small number of random graphs, the progenitors. #
# 		Choices: number of nodes per graph, number of edges per graph (or #
#           connection probability)#
# 		Log: features of graphs, plus a unique id, plus params of generation #
#           process#
# 		Save graph object#
pgid = new_run_ids$progenitor_graph_id#
progenitor <- randomDAG(num_nodes, prob_edges)#
#
# save and log aspects of the graph in a CSV:#
progenitor_graph_log = data.frame("progenitor_graph_id"=pgid,#
								  "sim_run_id"=new_run_ids$run_id,#
								  "num_nodes"=num_nodes,#
								  "prob_edges"=prob_edges,#
								  "num_edges"=length(unlist(edges(progenitor))))#
#
write.table(progenitor_graph_log, progenitor_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(progenitor_graph_log_file), #
    		col.names = "!"(file.exists(progenitor_graph_log_file)))#
#
# save and log an ASCII representation of the graph, and an R object:#
ascii_filename = paste(progenitor_graph_log_dir, "ASCII/progenitor_graph_",#
					   pgid, ".txt", sep="")#
r_filename = paste(progenitor_graph_log_dir, "R_objects/progenitor_graph_",#
				   pgid, ".R", sep="")#
save(progenitor, file=r_filename)#
dput(progenitor, file=ascii_filename)#
#
#################################################################################
# 	Take progenitor and evolve it for a few steps into a descendant. Repeat. #
# 		Choices: number of descendants, number of steps of evolution.#
# 		Log: features of graphs, plus a unique id, plus id of progenitor, plus #
#           params of evolution process, plus specific modifications made#
# 		Save graph object#
# 		At end of repeat: remove progenitor from memory#
#
evolved_list = list()#
for (i in 1:num_evolved){#
	evolved_list[[i]] <- evolveDAG(num_mods, progenitor)#
}#
#
target_graph = evolved_list[[1]][[1]]#
#
# save and log evolved graphs here#
first_ev_id = new_run_ids$evolved_graph_id#
last_ev_id = new_run_ids$evolved_graph_id + num_evolved - 1#
evolved_graph_ids = seq(first_ev_id, last_ev_id)#
#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
#
num_nodes=c()#
num_edges=c()#
mods = c()#
for (i in 1:num_evolved){#
	g = evolved_list[[i]][[1]]#
	num_nodes=c(num_nodes, numNodes(g))#
	num_edges=c(num_edges, length(unlist(edges(g))))#
	mods_i = paste(evolved_list[[i]][[2]], collapse="; ")#
	mods = c(mods, mods_i)#
	ascii_filename = paste(evolved_graph_log_dir, "ASCII/evolved_graph_",#
						   evolved_graph_ids[i], ".txt", sep="")#
	r_filename = paste(evolved_graph_log_dir, "R_objects/evolved_graph_",#
					   evolved_graph_ids[i], ".R", sep="")#
	save(g, file=r_filename)#
	dput(g, file=ascii_filename)#
}#
#
# todo: add params of evolution process, once those are not hard-coded#
evolved_graph_log = data.frame("evolved_graph_ids" = evolved_graph_ids,#
							"progenitor_id" = rep(pgid, num_evolved),#
							"sim_run_id" = rep(new_run_ids$run_id, num_evolved),#
							"num_nodes"= num_nodes,#
							"num_edges"= num_edges,#
							"num_mods" = num_mods,#
							"mods"= mods)#
write.table(evolved_graph_log, evolved_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(evolved_graph_log_file), #
    		col.names = "!"(file.exists(evolved_graph_log_file)))#
#
sim_run_log$evolved_graph_ids = paste(evolved_graph_ids, collapse=";")#
#
#################################################################################
# 	Generate data from each descendant graph. #
# 		Choices: parameterization, number of data points to generate.#
# 		Log: data filename, number of rows and columns, params of data #
#           generation, id of generator graph#
# 		Save data#
# 		At end of each data generation: remove graph from memory#
#
data_list = list()#
#
dataset_ids = evolved_graph_ids#
#
for (i in 1:num_evolved){#
	if (parameterization=="linear_gaussian"){#
		evolved_list[[i]][[1]] <- dag2gausspardag(evolved_list[[i]][[1]])#
	} else {#
		cat("error: parameterization not listed or not recognized\n")#
		break#
	}#
	data_list[[i]] <- rmvnorm.ivent(sample_size, evolved_list[[i]][[1]])#
	# save dataset#
	data_filename = paste(dataset_log_dir, "dataset_", dataset_ids[i], ".csv", #
						  sep="")#
	write.table(data_list[[i]], data_filename, sep=",", row.names=FALSE, #
				col.names=TRUE)#
}#
# log features of dataset#
# todo: add params of data generation#
# todo: log parameterized graph!!!#
dataset_log = data.frame("dataset_id" = dataset_ids,#
						 "evolved_graph_id"=evolved_graph_ids,#
						 "parameterization" = parameterization,#
						 "num_rows"=rep(sample_size, num_evolved),#
						 "num_columns"= num_nodes)#
#
write.table(dataset_log, dataset_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(dataset_log_file), #
    		col.names = "!"(file.exists(dataset_log_file)))#
#
sim_run_log$dataset_ids = paste(dataset_ids, collapse=";")#
#
# # if not doing evaluation in same loop, get rid of graphs:#
# rm(evolved_list) #
#
# todo: Getting some errors in this step - need to add some unit tests for the graph#
# modification procedure.#
# does duplicating a node cause problems? Maybe if you add an edge from the #
# duplicated node to other nodes? Need to check this.#
#
#################################################################################
# Data pooling:#
#
# 	Pool data from graphs descended from the same progenitor. #
# 		Choices: amount of data from each graph to include; number of graphs #
#           to include#
# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row #
#           numbers from the source data that were used in pooling, #
#
vars = colnames(data_list[[1]])#
rows_included_list = list()#
for (i in 1: num_evolved){#
	vars = intersect(vars, colnames(data_list[[i]]))#
	rows_included = sample(1:sample_size, pooled_sample_sizes[i])#
	rows_included_list[[i]] <- 1:sample_size %in% rows_included#
}#
# save and log the rows included and the vars included #
pool_data_list <- list()#
for (i in 1: num_evolved){#
	pool_data_list[[i]] <- subset(data_list[[i]], #
								  subset=rows_included_list[[i]], select=vars)#
}#
pool_data_frame = do.call(rbind, pool_data_list)#
#
pdid = new_run_ids$pooled_dataset_id#
#
pool_log = data.frame("pooled_dataset_id" = pdid,#
					  "component_dataset_ids" = paste(dataset_ids, collapse=";"),#
					  "pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))#
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")#
dput(rows_included_list, file=pool_filename)#
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(pooled_data_log_file), #
    		col.names = "!"(file.exists(pooled_data_log_file)))#
#
#################################################################################
# Graph search:#
#
# this is the hard part, the part where I want to interface with Tetrad!#
#
# 	Choose one graph from the descendants in each pool to be the "target". #
# 		Choice: which graph? Or repeat for all graphs? #
# 		Log: choice of target#
#
# 	Run search algorithm on the pooled data#
#       Choice: Which Tetrad jar? Which algorithms?#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and #
#           all parameters#
target_full_data = data_list[[1]]#
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])#
#
# start the JVM#
.jinit(path_to_tetrad_jar) #
#
# convert dataframes to tetrad dataset#
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)#
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)#
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)#
#
# initialize GES#
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)#
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)#
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)#
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)#
#
# search using GES#
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")#
#
# convert output of GES into an R object (graphNEL)#
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)
ges_graph_pooled
plot(ges_graph_pooled)
plot(target_graph
)
plot(ges_graph_pooled)
plot(target_graph)
plot(ges_graph_pooled)
plot(target_graph)
.jconstructors(ges_pooled)
.jmethods(ges_pooled)
tetrad_graph_pooled$getClass()
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))
ges_phase_2$getInitialGraph()
# search using GES#
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")#
#
# convert output of GES into an R object (graphNEL)#
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)
plot(ges_graph_phase_2)
?compareGraphs
compareGraphs(target_graph,ges_graph_phase_2)
new_run_ids
recovered_graph_log_dir = "../log/recovered_graphs/"
target_graph
ges_graph_pooled
compareGraphs(target_graph, ges_graph_pooled)
search_log_file="../log/search_log.txt"
eval <- data_frame(compareGraphs(ges_graph_solo, target_graph))
eval <- data.frame(compareGraphs(ges_graph_solo, target_graph))
target_full_data = data_list[[1]]#
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])#
#
# start the JVM#
.jinit(path_to_tetrad_jar) #
#
# convert dataframes to tetrad dataset#
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)#
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)#
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)
rgid = new_run_ids$recovered_graph_id
rgid
if (file.exists(last_id_log_file)){#
	last_run_ids = read.table(last_id_log_file, header=TRUE, sep=",")#
} else {#
	last_run_ids = data.frame("run_id"=0, "progenitor_graph_id"=0, #
							  "evolved_graph_id"=0, "dataset_id"=0, #
 							  "pooled_dataset_id"=0, "search_id"=0,#
 							  "recovered_graph_id"=0, "eval_id"=0)#
}#
new_run_ids = last_run_ids + 1
new_run_ids
rgid = new_run_ids$recovered_graph_id#
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)#
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)#
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)#
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)#
# search using GES#
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgid, ".txt", sep="")#
dput(ges_graph_pooled, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgid, ".txt", sep="")#
save(ges_graph_pooled, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgid, ".txt", sep="")#
save(tetrad_graph_pooled, file=rjava_object_file)
search_log = data.frame(recovered_graph_id = rgid,#
						type="pooled_data",#
						dataset_id=pdid,#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))
rgid = rgid + 1#
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))#
# search using GES#
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgid, ".txt", sep="")#
dput(ges_graph_phase_2, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgid, ".R", sep="")#
save(ges_graph_phase_2, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgid, ".R", sep="")#
save(tetrad_graph_phase_2, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgid,#
						type="phase_2",#
						dataset_id=min(dataset_ids),#
						prior_graph=rgid-1,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))
rgid = rgid + 1#
ges_solo = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
# search using GES#
tetrad_graph_solo = .jcall(ges_solo, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_solo = tetradPattern2graphNEL(tetrad_graph_solo)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgid, ".txt", sep="")#
dput(ges_graph_solo, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgid, ".R", sep="")#
save(ges_graph_solo, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgid, ".R", sep="")#
save(tetrad_graph_solo, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgid,#
						type="target_small_data",#
						dataset_id=min(dataset_ids),#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))
eval <- data.frame(compareGraphs(ges_graph_solo, target_graph))
eval
eval <- compareGraphs(ges_graph_solo, target_graph)
eval
is.data.frame(eval)
eval <- rbind(compareGraphs(ges_graph_solo, target_graph),#
			  compareGraphs(ges_graph_phase_2, target_graph))
eval
is.data.frame(eval)
is.matrix(eval)
eval <- data.frame(eval)
eval
eval$eval_ids <- seq(new_run_ids$eval_id, new_run_ids$eval_id + nrow(eval) -1)
eval
rgids = seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)
rgids
eval_log_file = "../log/eval_log.txt"
new_run_ids
evolved_graph_ids
dataset_id
#################################################################################
# Simulation pipeline:#
#
# load required libraries#
library(graph)#
library(RBGL)#
library(rJava)#
library(pcalg)#
library(stringr)#
# library(gRbase)#
# library(dplyr)#
#
# load configuration file#
setwd("/Users/lizzie/Dissertation_code/Simulation/R-scripts")#
source("simulation_config.R")#
source("graph-rewriting-grammar.R")#
#
#################################################################################
# Load unique ids of last run, if this is not the first run:#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
if (file.exists(last_id_log_file)){#
	last_run_ids = read.table(last_id_log_file, header=TRUE, sep=",")#
} else {#
	last_run_ids = data.frame("run_id"=0, "progenitor_graph_id"=0, #
							  "evolved_graph_id"=0, "dataset_id"=0, #
 							  "pooled_dataset_id"=0, "search_id"=0,#
 							  "recovered_graph_id"=0, "eval_id"=0)#
}#
new_run_ids = last_run_ids + 1#
#
#################################################################################
# file to log every run of the pipeline#
# should contain: run id, date, progenitor graph id, #
#     list of evolved graph ids, list of dataset ids, list of pooled data ids, #
#     list of search ids, list of evaluation set ids, start time, end time,#
#     duration#
sim_run_log = data.frame("run_id"=new_run_ids$run_id, #
						 "start_time"=Sys.time(),#
						 "end_time"=NA,#
						 "duration"=NA,#
						 "progenitor_graph_id"=new_run_ids$progenitor_graph_id,#
						 "evolved_graph_ids"=NA,#
						 "dataset_ids"=NA,#
						 "pooled_data_id"=new_run_ids$pooled_dataset_id,#
						 "target_graph_id"=new_run_ids$evolved_graph_id,#
						 "search_ids"=NA,#
						 "evaluation_ids"=NA)#
#
#################################################################################
# Graph generation:#
# 	Generate small number of random graphs, the progenitors. #
# 		Choices: number of nodes per graph, number of edges per graph (or #
#           connection probability)#
# 		Log: features of graphs, plus a unique id, plus params of generation #
#           process#
# 		Save graph object#
pgid = new_run_ids$progenitor_graph_id#
progenitor <- randomDAG(num_nodes, prob_edges)#
#
# save and log aspects of the graph in a CSV:#
progenitor_graph_log = data.frame("progenitor_graph_id"=pgid,#
								  "sim_run_id"=new_run_ids$run_id,#
								  "num_nodes"=num_nodes,#
								  "prob_edges"=prob_edges,#
								  "num_edges"=length(unlist(edges(progenitor))))#
#
write.table(progenitor_graph_log, progenitor_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(progenitor_graph_log_file), #
    		col.names = "!"(file.exists(progenitor_graph_log_file)))#
#
# save and log an ASCII representation of the graph, and an R object:#
ascii_filename = paste(progenitor_graph_log_dir, "ASCII/progenitor_graph_",#
					   pgid, ".txt", sep="")#
r_filename = paste(progenitor_graph_log_dir, "R_objects/progenitor_graph_",#
				   pgid, ".R", sep="")#
save(progenitor, file=r_filename)#
dput(progenitor, file=ascii_filename)#
#
#################################################################################
# 	Take progenitor and evolve it for a few steps into a descendant. Repeat. #
# 		Choices: number of descendants, number of steps of evolution.#
# 		Log: features of graphs, plus a unique id, plus id of progenitor, plus #
#           params of evolution process, plus specific modifications made#
# 		Save graph object#
# 		At end of repeat: remove progenitor from memory#
#
evolved_list = list()#
for (i in 1:num_evolved){#
	evolved_list[[i]] <- evolveDAG(num_mods, progenitor)#
}#
#
target_graph = evolved_list[[1]][[1]]#
#
# save and log evolved graphs here#
first_ev_id = new_run_ids$evolved_graph_id#
last_ev_id = new_run_ids$evolved_graph_id + num_evolved - 1#
evolved_graph_ids = seq(first_ev_id, last_ev_id)#
#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
#
num_nodes=c()#
num_edges=c()#
mods = c()#
for (i in 1:num_evolved){#
	g = evolved_list[[i]][[1]]#
	num_nodes=c(num_nodes, numNodes(g))#
	num_edges=c(num_edges, length(unlist(edges(g))))#
	mods_i = paste(evolved_list[[i]][[2]], collapse="; ")#
	mods = c(mods, mods_i)#
	ascii_filename = paste(evolved_graph_log_dir, "ASCII/evolved_graph_",#
						   evolved_graph_ids[i], ".txt", sep="")#
	r_filename = paste(evolved_graph_log_dir, "R_objects/evolved_graph_",#
					   evolved_graph_ids[i], ".R", sep="")#
	save(g, file=r_filename)#
	dput(g, file=ascii_filename)#
}#
#
# todo: add params of evolution process, once those are not hard-coded#
evolved_graph_log = data.frame("evolved_graph_ids" = evolved_graph_ids,#
							"progenitor_id" = rep(pgid, num_evolved),#
							"sim_run_id" = rep(new_run_ids$run_id, num_evolved),#
							"num_nodes"= num_nodes,#
							"num_edges"= num_edges,#
							"num_mods" = num_mods,#
							"mods"= mods)#
write.table(evolved_graph_log, evolved_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(evolved_graph_log_file), #
    		col.names = "!"(file.exists(evolved_graph_log_file)))#
#
sim_run_log$evolved_graph_ids = paste(evolved_graph_ids, collapse=";")
data_list = list()#
#
dataset_ids = evolved_graph_ids#
#
for (i in 1:num_evolved){#
	if (parameterization=="linear_gaussian"){#
		evolved_list[[i]][[1]] <- dag2gausspardag(evolved_list[[i]][[1]])#
	} else {#
		cat("error: parameterization not listed or not recognized\n")#
		break#
	}#
	data_list[[i]] <- rmvnorm.ivent(sample_size, evolved_list[[i]][[1]])#
	# save dataset#
	data_filename = paste(dataset_log_dir, "dataset_", dataset_ids[i], ".csv", #
						  sep="")#
	write.table(data_list[[i]], data_filename, sep=",", row.names=FALSE, #
				col.names=TRUE)#
}#
# log features of dataset#
# todo: add params of data generation#
# todo: log parameterized graph!!!#
dataset_log = data.frame("dataset_id" = dataset_ids,#
						 "evolved_graph_id"=evolved_graph_ids,#
						 "parameterization" = parameterization,#
						 "num_rows"=rep(sample_size, num_evolved),#
						 "num_columns"= num_nodes)#
#
write.table(dataset_log, dataset_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(dataset_log_file), #
    		col.names = "!"(file.exists(dataset_log_file)))#
#
sim_run_log$dataset_ids = paste(dataset_ids, collapse=";")#
#
# # if not doing evaluation in same loop, get rid of graphs:#
# rm(evolved_list) #
#
# todo: Getting some errors in this step - need to add some unit tests for the graph#
# modification procedure.#
# does duplicating a node cause problems? Maybe if you add an edge from the #
# duplicated node to other nodes? Need to check this.#
#
#################################################################################
# Data pooling:#
#
# 	Pool data from graphs descended from the same progenitor. #
# 		Choices: amount of data from each graph to include; number of graphs #
#           to include#
# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row #
#           numbers from the source data that were used in pooling, #
#
vars = colnames(data_list[[1]])#
rows_included_list = list()#
for (i in 1: num_evolved){#
	vars = intersect(vars, colnames(data_list[[i]]))#
	rows_included = sample(1:sample_size, pooled_sample_sizes[i])#
	rows_included_list[[i]] <- 1:sample_size %in% rows_included#
}#
# save and log the rows included and the vars included #
pool_data_list <- list()#
for (i in 1: num_evolved){#
	pool_data_list[[i]] <- subset(data_list[[i]], #
								  subset=rows_included_list[[i]], select=vars)#
}#
pool_data_frame = do.call(rbind, pool_data_list)#
#
pdid = new_run_ids$pooled_dataset_id#
#
pool_log = data.frame("pooled_dataset_id" = pdid,#
					  "component_dataset_ids" = paste(dataset_ids, collapse=";"),#
					  "pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))#
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")#
dput(rows_included_list, file=pool_filename)#
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(pooled_data_log_file), #
    		col.names = "!"(file.exists(pooled_data_log_file)))#
#
#################################################################################
# Graph search:#
#
# this is the hard part, the part where I want to interface with Tetrad!#
#
# 	Choose one graph from the descendants in each pool to be the "target". #
# 		Choice: which graph? Or repeat for all graphs? #
# 		Log: choice of target#
#
# 	Run search algorithm on the pooled data#
#       Choice: Which Tetrad jar? Which algorithms?#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and #
#           all parameters
target_full_data = data_list[[1]]#
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])#
#
# start the JVM#
.jinit(path_to_tetrad_jar) #
#
# convert dataframes to tetrad dataset#
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)#
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)#
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)#
#
rgids = seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)#
#
############################
# First search: Pooled data#
# initialize GES#
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)#
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)#
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)#
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)#
# search using GES#
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgid, ".txt", sep="")#
dput(ges_graph_pooled, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgid, ".R", sep="")#
save(ges_graph_pooled, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgid, ".R", sep="")#
save(tetrad_graph_pooled, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[1],#
						type="pooled_data",#
						dataset_id=pdid,#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
#
############################
# Second search: Starting point + target data#
rgid = rgid + 1#
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))#
# search using GES#
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgid, ".txt", sep="")#
dput(ges_graph_phase_2, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgid, ".R", sep="")#
save(ges_graph_phase_2, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgid, ".R", sep="")#
save(tetrad_graph_phase_2, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[2],#
						type="phase_2",#
						dataset_id=min(dataset_ids),#
						prior_graph=rgid-1,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
############################
# Third search: Target data alone#
rgid = rgid + 1#
ges_solo = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
# search using GES#
tetrad_graph_solo = .jcall(ges_solo, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_solo = tetradPattern2graphNEL(tetrad_graph_solo)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgid, ".txt", sep="")#
dput(ges_graph_solo, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgid, ".R", sep="")#
save(ges_graph_solo, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgid, ".R", sep="")#
save(tetrad_graph_solo, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[3],#
						type="target_small_data",#
						dataset_id=min(dataset_ids),#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
eval <- rbind(compareGraphs(ges_graph_solo, target_graph),#
			  compareGraphs(ges_graph_phase_2, target_graph))#
eval <- data.frame(eval)#
eval$eval_ids <- seq(new_run_ids$eval_id, new_run_ids$eval_id + nrow(eval) -1)#
eval$true_graph_id <- rep(min(evolved_graph_ids), nrow(eval))#
eval$recovered_graph_id <- rgids[c(2,3)]#
write.table(eval, eval_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(eval_log_file), #
    		col.names = "!"(file.exists(eval_log_file)))
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
new_run_ids$dataset_id = max(evolved_graph_ids)#
new_run_ids$search_id = max(rgids)#
new_run_ids$recovered_graph_id = max(rgids)#
new_run_ids$eval_id = max(eval$eval_ids)#
write.table(new_run_ids, last_id_log_file, sep=",", row.names=FALSE, #
    			append=FALSE, col.names = TRUE)
# log the simulation run#
sim_run_log$end_time <- Sys.time()#
sim_run_log$duration <- sim_run_log$end_time - sim_run_log$start_time#
write.table(sim_run_log, sim_run_log_file, sep=",", row.names=FALSE, #
    append=file.exists(sim_run_log_file), #
    col.names = "!"(file.exists(sim_run_log_file)))
#################################################################################
# Simulation pipeline:#
#
# load required libraries#
library(graph)#
library(RBGL)#
library(rJava)#
library(pcalg)#
library(stringr)#
# library(gRbase)#
# library(dplyr)#
#
# load configuration file#
setwd("/Users/lizzie/Dissertation_code/Simulation/R-scripts")#
source("simulation_config.R")#
source("graph-rewriting-grammar.R")#
#
#################################################################################
# Load unique ids of last run, if this is not the first run:#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
if (file.exists(last_id_log_file)){#
	last_run_ids = read.table(last_id_log_file, header=TRUE, sep=",")#
} else {#
	last_run_ids = data.frame("run_id"=0, "progenitor_graph_id"=0, #
							  "evolved_graph_id"=0, "dataset_id"=0, #
 							  "pooled_dataset_id"=0, "search_id"=0,#
 							  "recovered_graph_id"=0, "eval_id"=0)#
}#
new_run_ids = last_run_ids + 1#
#
#################################################################################
# file to log every run of the pipeline#
# should contain: run id, date, progenitor graph id, #
#     list of evolved graph ids, list of dataset ids, list of pooled data ids, #
#     list of search ids, list of evaluation set ids, start time, end time,#
#     duration#
sim_run_log = data.frame("run_id"=new_run_ids$run_id, #
						 "start_time"=Sys.time(),#
						 "end_time"=NA,#
						 "duration"=NA,#
						 "progenitor_graph_id"=new_run_ids$progenitor_graph_id,#
						 "evolved_graph_ids"=NA,#
						 "dataset_ids"=NA,#
						 "pooled_data_id"=new_run_ids$pooled_dataset_id,#
						 "target_graph_id"=new_run_ids$evolved_graph_id,#
						 "search_ids"=NA,#
						 "evaluation_ids"=NA)#
#
#################################################################################
# Graph generation:#
# 	Generate small number of random graphs, the progenitors. #
# 		Choices: number of nodes per graph, number of edges per graph (or #
#           connection probability)#
# 		Log: features of graphs, plus a unique id, plus params of generation #
#           process#
# 		Save graph object#
pgid = new_run_ids$progenitor_graph_id#
progenitor <- randomDAG(num_nodes, prob_edges)#
#
# save and log aspects of the graph in a CSV:#
progenitor_graph_log = data.frame("progenitor_graph_id"=pgid,#
								  "sim_run_id"=new_run_ids$run_id,#
								  "num_nodes"=num_nodes,#
								  "prob_edges"=prob_edges,#
								  "num_edges"=length(unlist(edges(progenitor))))#
#
write.table(progenitor_graph_log, progenitor_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(progenitor_graph_log_file), #
    		col.names = "!"(file.exists(progenitor_graph_log_file)))#
#
# save and log an ASCII representation of the graph, and an R object:#
ascii_filename = paste(progenitor_graph_log_dir, "ASCII/progenitor_graph_",#
					   pgid, ".txt", sep="")#
r_filename = paste(progenitor_graph_log_dir, "R_objects/progenitor_graph_",#
				   pgid, ".R", sep="")#
save(progenitor, file=r_filename)#
dput(progenitor, file=ascii_filename)#
#
#################################################################################
# 	Take progenitor and evolve it for a few steps into a descendant. Repeat. #
# 		Choices: number of descendants, number of steps of evolution.#
# 		Log: features of graphs, plus a unique id, plus id of progenitor, plus #
#           params of evolution process, plus specific modifications made#
# 		Save graph object#
# 		At end of repeat: remove progenitor from memory#
#
evolved_list = list()#
for (i in 1:num_evolved){#
	evolved_list[[i]] <- evolveDAG(num_mods, progenitor)#
}#
#
target_graph = evolved_list[[1]][[1]]#
#
# save and log evolved graphs here#
first_ev_id = new_run_ids$evolved_graph_id#
last_ev_id = new_run_ids$evolved_graph_id + num_evolved - 1#
evolved_graph_ids = seq(first_ev_id, last_ev_id)#
#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
#
num_nodes=c()#
num_edges=c()#
mods = c()#
for (i in 1:num_evolved){#
	g = evolved_list[[i]][[1]]#
	num_nodes=c(num_nodes, numNodes(g))#
	num_edges=c(num_edges, length(unlist(edges(g))))#
	mods_i = paste(evolved_list[[i]][[2]], collapse="; ")#
	mods = c(mods, mods_i)#
	ascii_filename = paste(evolved_graph_log_dir, "ASCII/evolved_graph_",#
						   evolved_graph_ids[i], ".txt", sep="")#
	r_filename = paste(evolved_graph_log_dir, "R_objects/evolved_graph_",#
					   evolved_graph_ids[i], ".R", sep="")#
	save(g, file=r_filename)#
	dput(g, file=ascii_filename)#
}#
#
# todo: add params of evolution process, once those are not hard-coded#
evolved_graph_log = data.frame("evolved_graph_ids" = evolved_graph_ids,#
							"progenitor_id" = rep(pgid, num_evolved),#
							"sim_run_id" = rep(new_run_ids$run_id, num_evolved),#
							"num_nodes"= num_nodes,#
							"num_edges"= num_edges,#
							"num_mods" = num_mods,#
							"mods"= mods)#
write.table(evolved_graph_log, evolved_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(evolved_graph_log_file), #
    		col.names = "!"(file.exists(evolved_graph_log_file)))#
#
sim_run_log$evolved_graph_ids = paste(evolved_graph_ids, collapse=";")#
#
#################################################################################
# 	Generate data from each descendant graph. #
# 		Choices: parameterization, number of data points to generate.#
# 		Log: data filename, number of rows and columns, params of data #
#           generation, id of generator graph#
# 		Save data#
# 		At end of each data generation: remove graph from memory#
#
data_list = list()#
#
dataset_ids = evolved_graph_ids#
#
for (i in 1:num_evolved){#
	if (parameterization=="linear_gaussian"){#
		evolved_list[[i]][[1]] <- dag2gausspardag(evolved_list[[i]][[1]])#
	} else {#
		cat("error: parameterization not listed or not recognized\n")#
		break#
	}#
	data_list[[i]] <- rmvnorm.ivent(sample_size, evolved_list[[i]][[1]])#
	# save dataset#
	data_filename = paste(dataset_log_dir, "dataset_", dataset_ids[i], ".csv", #
						  sep="")#
	write.table(data_list[[i]], data_filename, sep=",", row.names=FALSE, #
				col.names=TRUE)#
}#
# log features of dataset#
# todo: add params of data generation#
# todo: log parameterized graph!!!#
dataset_log = data.frame("dataset_id" = dataset_ids,#
						 "evolved_graph_id"=evolved_graph_ids,#
						 "parameterization" = parameterization,#
						 "num_rows"=rep(sample_size, num_evolved),#
						 "num_columns"= num_nodes)#
#
write.table(dataset_log, dataset_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(dataset_log_file), #
    		col.names = "!"(file.exists(dataset_log_file)))#
#
sim_run_log$dataset_ids = paste(dataset_ids, collapse=";")#
#
# # if not doing evaluation in same loop, get rid of graphs:#
# rm(evolved_list) #
#
# todo: Getting some errors in this step - need to add some unit tests for the graph#
# modification procedure.#
# does duplicating a node cause problems? Maybe if you add an edge from the #
# duplicated node to other nodes? Need to check this.#
#
#################################################################################
# Data pooling:#
#
# 	Pool data from graphs descended from the same progenitor. #
# 		Choices: amount of data from each graph to include; number of graphs #
#           to include#
# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row #
#           numbers from the source data that were used in pooling, #
#
vars = colnames(data_list[[1]])#
rows_included_list = list()#
for (i in 1: num_evolved){#
	vars = intersect(vars, colnames(data_list[[i]]))#
	rows_included = sample(1:sample_size, pooled_sample_sizes[i])#
	rows_included_list[[i]] <- 1:sample_size %in% rows_included#
}#
# save and log the rows included and the vars included #
pool_data_list <- list()#
for (i in 1: num_evolved){#
	pool_data_list[[i]] <- subset(data_list[[i]], #
								  subset=rows_included_list[[i]], select=vars)#
}#
pool_data_frame = do.call(rbind, pool_data_list)#
#
pdid = new_run_ids$pooled_dataset_id#
#
pool_log = data.frame("pooled_dataset_id" = pdid,#
					  "component_dataset_ids" = paste(dataset_ids, collapse=";"),#
					  "pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))#
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")#
dput(rows_included_list, file=pool_filename)#
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(pooled_data_log_file), #
    		col.names = "!"(file.exists(pooled_data_log_file)))#
#
#################################################################################
# Graph search:#
#
# this is the hard part, the part where I want to interface with Tetrad!#
#
# 	Choose one graph from the descendants in each pool to be the "target". #
# 		Choice: which graph? Or repeat for all graphs? #
# 		Log: choice of target#
#
# 	Run search algorithm on the pooled data#
#       Choice: Which Tetrad jar? Which algorithms?#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and #
#           all parameters#
target_full_data = data_list[[1]]#
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])#
#
# start the JVM#
.jinit(path_to_tetrad_jar) #
#
# convert dataframes to tetrad dataset#
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)#
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)#
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)#
#
rgids = seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)#
#
############################
# First search: Pooled data#
# initialize GES#
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)#
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)#
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)#
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)#
# search using GES#
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_pooled, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_pooled, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_pooled, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[1],#
						type="pooled_data",#
						dataset_id=pdid,#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
#
############################
# Second search: Starting point + target data#
rgid = rgid + 1#
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))#
# search using GES#
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_phase_2, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_phase_2, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_phase_2, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[2],#
						type="phase_2",#
						dataset_id=min(dataset_ids),#
						prior_graph=rgid-1,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
############################
# Third search: Target data alone#
rgid = rgid + 1#
ges_solo = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
# search using GES#
tetrad_graph_solo = .jcall(ges_solo, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_solo = tetradPattern2graphNEL(tetrad_graph_solo)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_solo, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_solo, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_solo, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[3],#
						type="target_small_data",#
						dataset_id=min(dataset_ids),#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
eval <- rbind(compareGraphs(ges_graph_solo, target_graph),#
			  compareGraphs(ges_graph_phase_2, target_graph))#
eval <- data.frame(eval)#
eval$eval_ids <- seq(new_run_ids$eval_id, new_run_ids$eval_id + nrow(eval) -1)#
eval$true_graph_id <- rep(min(evolved_graph_ids), nrow(eval))#
eval$recovered_graph_id <- rgids[c(2,3)]#
write.table(eval, eval_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(eval_log_file), #
    		col.names = "!"(file.exists(eval_log_file)))#
#
# todo: log features of graph and parameters of search#
#
#   use resulting graph as (a) a prior, and (b) a starting point for second phase of search#
#   log the prior penalty type, the prior/starting point graph, the data used for the second phase#
#
# 	Run search algorithm on data from individual target graph:#
# 	(1) using just the rows that went into the pooled data#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters#
# 	(2) using a superset, including the pooled data but also additional rows, so that the sample size is equal to the sample size of the pooled data.#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters#
# Evaluation:#
#
# todo: #
# [DONE] 1. set up all logging#
# [DONE] 2. two-stage search #
# 3. prior network search#
# [DONE] 4. evaluate results#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
new_run_ids$dataset_id = max(evolved_graph_ids)#
new_run_ids$search_id = max(rgids)#
new_run_ids$recovered_graph_id = max(rgids)#
new_run_ids$eval_id = max(eval$eval_ids)#
write.table(new_run_ids, last_id_log_file, sep=",", row.names=FALSE, #
    			append=FALSE, col.names = TRUE)#
# log the simulation run#
sim_run_log$end_time <- Sys.time()#
sim_run_log$duration <- sim_run_log$end_time - sim_run_log$start_time#
write.table(sim_run_log, sim_run_log_file, sep=",", row.names=FALSE, #
    append=file.exists(sim_run_log_file), #
    col.names = "!"(file.exists(sim_run_log_file)))
#################################################################################
# Simulation pipeline:#
#
# load required libraries#
library(graph)#
library(RBGL)#
library(rJava)#
library(pcalg)#
library(stringr)#
# library(gRbase)#
# library(dplyr)#
#
# load configuration file#
setwd("/Users/lizzie/Dissertation_code/Simulation/R-scripts")#
source("simulation_config.R")#
source("graph-rewriting-grammar.R")#
source("logger.R")#
#
#################################################################################
# Load unique ids of last run, if this is not the first run:#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
new_run_ids <- get_new_run_ids(last_id_log_file)#
#
#################################################################################
# file to log every run of the pipeline#
# should contain: run id, date, progenitor graph id, #
#     list of evolved graph ids, list of dataset ids, list of pooled data ids, #
#     list of search ids, list of evaluation set ids, start time, end time,#
#     duration#
sim_run_log = data.frame("run_id"=new_run_ids$run_id, #
						 "start_time"=Sys.time(),#
						 "end_time"=NA,#
						 "duration"=NA,#
						 "progenitor_graph_id"=new_run_ids$progenitor_graph_id,#
						 "evolved_graph_ids"=NA,#
						 "dataset_ids"=NA,#
						 "pooled_data_id"=new_run_ids$pooled_dataset_id,#
						 "target_graph_id"=new_run_ids$evolved_graph_id,#
						 "search_ids"=NA,#
						 "evaluation_ids"=NA)#
#
#################################################################################
# Graph generation:#
# 	Generate small number of random graphs, the progenitors. #
# 		Choices: number of nodes per graph, number of edges per graph (or #
#           connection probability)#
# 		Log: features of graphs, plus a unique id, plus params of generation #
#           process#
# 		Save graph object#
#
progenitor <- randomDAG(num_nodes, prob_edges)#
#
# save and log aspects of the graph in a CSV:#
pgid <- log_progenitor(progenitor, new_run_ids, num_nodes, prob_edges,#
					   progenitor_graph_log_file, progenitor_graph_log_dir)#
#
#################################################################################
# 	Take progenitor and evolve it for a few steps into a descendant. Repeat. #
# 		Choices: number of descendants, number of steps of evolution.#
# 		Log: features of graphs, plus a unique id, plus id of progenitor, plus #
#           params of evolution process, plus specific modifications made#
# 		Save graph object#
# 		At end of repeat: remove progenitor from memory#
#
evolved_list = list()#
for (i in 1:num_evolved){#
	evolved_list[[i]] <- evolveDAG(num_mods, progenitor)#
}#
#
target_graph = evolved_list[[1]][[1]]#
#
# save and log evolved graphs here#
first_ev_id = new_run_ids$evolved_graph_id#
last_ev_id = new_run_ids$evolved_graph_id + num_evolved - 1#
evolved_graph_ids = seq(first_ev_id, last_ev_id)#
#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
#
num_nodes=c()#
num_edges=c()#
mods = c()#
for (i in 1:num_evolved){#
	g = evolved_list[[i]][[1]]#
	num_nodes=c(num_nodes, numNodes(g))#
	num_edges=c(num_edges, length(unlist(edges(g))))#
	mods_i = paste(evolved_list[[i]][[2]], collapse="; ")#
	mods = c(mods, mods_i)#
	ascii_filename = paste(evolved_graph_log_dir, "ASCII/evolved_graph_",#
						   evolved_graph_ids[i], ".txt", sep="")#
	r_filename = paste(evolved_graph_log_dir, "R_objects/evolved_graph_",#
					   evolved_graph_ids[i], ".R", sep="")#
	save(g, file=r_filename)#
	dput(g, file=ascii_filename)#
}#
#
# todo: add params of evolution process, once those are not hard-coded#
evolved_graph_log = data.frame("evolved_graph_ids" = evolved_graph_ids,#
							"progenitor_id" = rep(pgid, num_evolved),#
							"sim_run_id" = rep(new_run_ids$run_id, num_evolved),#
							"num_nodes"= num_nodes,#
							"num_edges"= num_edges,#
							"num_mods" = num_mods,#
							"mods"= mods)#
write.table(evolved_graph_log, evolved_graph_log_file, sep=",", #
			row.names=FALSE, #
    		append=file.exists(evolved_graph_log_file), #
    		col.names = "!"(file.exists(evolved_graph_log_file)))#
#
sim_run_log$evolved_graph_ids = paste(evolved_graph_ids, collapse=";")#
#
#################################################################################
# 	Generate data from each descendant graph. #
# 		Choices: parameterization, number of data points to generate.#
# 		Log: data filename, number of rows and columns, params of data #
#           generation, id of generator graph#
# 		Save data#
# 		At end of each data generation: remove graph from memory#
#
data_list = list()#
#
dataset_ids = evolved_graph_ids#
#
for (i in 1:num_evolved){#
	if (parameterization=="linear_gaussian"){#
		evolved_list[[i]][[1]] <- dag2gausspardag(evolved_list[[i]][[1]])#
	} else {#
		cat("error: parameterization not listed or not recognized\n")#
		break#
	}#
	data_list[[i]] <- rmvnorm.ivent(sample_size, evolved_list[[i]][[1]])#
	# save dataset#
	data_filename = paste(dataset_log_dir, "dataset_", dataset_ids[i], ".csv", #
						  sep="")#
	write.table(data_list[[i]], data_filename, sep=",", row.names=FALSE, #
				col.names=TRUE)#
}#
# log features of dataset#
# todo: add params of data generation#
# todo: log parameterized graph!!!#
dataset_log = data.frame("dataset_id" = dataset_ids,#
						 "evolved_graph_id"=evolved_graph_ids,#
						 "parameterization" = parameterization,#
						 "num_rows"=rep(sample_size, num_evolved),#
						 "num_columns"= num_nodes)#
#
write.table(dataset_log, dataset_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(dataset_log_file), #
    		col.names = "!"(file.exists(dataset_log_file)))#
#
sim_run_log$dataset_ids = paste(dataset_ids, collapse=";")#
#
# # if not doing evaluation in same loop, get rid of graphs:#
# rm(evolved_list) #
#
# todo: Getting some errors in this step - need to add some unit tests for the graph#
# modification procedure.#
# does duplicating a node cause problems? Maybe if you add an edge from the #
# duplicated node to other nodes? Need to check this.#
#
#################################################################################
# Data pooling:#
#
# 	Pool data from graphs descended from the same progenitor. #
# 		Choices: amount of data from each graph to include; number of graphs #
#           to include#
# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row #
#           numbers from the source data that were used in pooling, #
#
vars = colnames(data_list[[1]])#
rows_included_list = list()#
for (i in 1: num_evolved){#
	vars = intersect(vars, colnames(data_list[[i]]))#
	rows_included = sample(1:sample_size, pooled_sample_sizes[i])#
	rows_included_list[[i]] <- 1:sample_size %in% rows_included#
}#
# save and log the rows included and the vars included #
pool_data_list <- list()#
for (i in 1: num_evolved){#
	pool_data_list[[i]] <- subset(data_list[[i]], #
								  subset=rows_included_list[[i]], select=vars)#
}#
pool_data_frame = do.call(rbind, pool_data_list)#
#
pdid = new_run_ids$pooled_dataset_id#
#
pool_log = data.frame("pooled_dataset_id" = pdid,#
					  "component_dataset_ids" = paste(dataset_ids, collapse=";"),#
					  "pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))#
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")#
dput(rows_included_list, file=pool_filename)#
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(pooled_data_log_file), #
    		col.names = "!"(file.exists(pooled_data_log_file)))#
#
#################################################################################
# Graph search:#
#
# this is the hard part, the part where I want to interface with Tetrad!#
#
# 	Choose one graph from the descendants in each pool to be the "target". #
# 		Choice: which graph? Or repeat for all graphs? #
# 		Log: choice of target#
#
# 	Run search algorithm on the pooled data#
#       Choice: Which Tetrad jar? Which algorithms?#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and #
#           all parameters#
target_full_data = data_list[[1]]#
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])#
#
# start the JVM#
.jinit(path_to_tetrad_jar) #
#
# convert dataframes to tetrad dataset#
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)#
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)#
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)#
#
rgids = seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)#
#
############################
# First search: Pooled data#
# initialize GES#
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)#
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)#
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)#
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)#
# search using GES#
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_pooled, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_pooled, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_pooled, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[1],#
						type="pooled_data",#
						dataset_id=pdid,#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
#
############################
# Second search: Starting point + target data#
rgid = rgid + 1#
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))#
# search using GES#
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_phase_2, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_phase_2, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_phase_2, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[2],#
						type="phase_2",#
						dataset_id=min(dataset_ids),#
						prior_graph=rgid-1,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
############################
# Third search: Target data alone#
rgid = rgid + 1#
ges_solo = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
# search using GES#
tetrad_graph_solo = .jcall(ges_solo, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_solo = tetradPattern2graphNEL(tetrad_graph_solo)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_solo, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_solo, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_solo, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[3],#
						type="target_small_data",#
						dataset_id=min(dataset_ids),#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
eval <- rbind(compareGraphs(ges_graph_solo, target_graph),#
			  compareGraphs(ges_graph_phase_2, target_graph))#
eval <- data.frame(eval)#
eval$eval_ids <- seq(new_run_ids$eval_id, new_run_ids$eval_id + nrow(eval) -1)#
eval$true_graph_id <- rep(min(evolved_graph_ids), nrow(eval))#
eval$recovered_graph_id <- rgids[c(2,3)]#
write.table(eval, eval_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(eval_log_file), #
    		col.names = "!"(file.exists(eval_log_file)))#
#
# todo: log features of graph and parameters of search#
#
#   use resulting graph as (a) a prior, and (b) a starting point for second phase of search#
#   log the prior penalty type, the prior/starting point graph, the data used for the second phase#
#
# 	Run search algorithm on data from individual target graph:#
# 	(1) using just the rows that went into the pooled data#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters#
# 	(2) using a superset, including the pooled data but also additional rows, so that the sample size is equal to the sample size of the pooled data.#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters#
# Evaluation:#
#
# todo: #
# [DONE] 1. set up all logging#
# [DONE] 2. two-stage search #
# 3. prior network search#
# [DONE] 4. evaluate results#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
new_run_ids$dataset_id = max(evolved_graph_ids)#
new_run_ids$search_id = max(rgids)#
new_run_ids$recovered_graph_id = max(rgids)#
new_run_ids$eval_id = max(eval$eval_ids)#
write.table(new_run_ids, last_id_log_file, sep=",", row.names=FALSE, #
    			append=FALSE, col.names = TRUE)#
# log the simulation run#
sim_run_log$end_time <- Sys.time()#
sim_run_log$duration <- sim_run_log$end_time - sim_run_log$start_time#
write.table(sim_run_log, sim_run_log_file, sep=",", row.names=FALSE, #
    append=file.exists(sim_run_log_file), #
    col.names = "!"(file.exists(sim_run_log_file)))
length(evolved_list)
log_evolved(evolved_list, new_run_ids, pgid, num_evolved, evolved_graph_log_file)
log_evolved <- function(evolved_list, new_run_ids, pgid, num_evolved, evolved_graph_log_file){#
	# save and log evolved graphs here#
	first_ev_id = new_run_ids$evolved_graph_id#
	last_ev_id = new_run_ids$evolved_graph_id + num_evolved - 1#
	evolved_graph_ids = seq(first_ev_id, last_ev_id)#
	num_nodes=c()#
	num_edges=c()#
	mods = c()#
	for (i in 1:num_evolved){#
		g = evolved_list[[i]][[1]]#
		num_nodes=c(num_nodes, numNodes(g))#
		num_edges=c(num_edges, length(unlist(edges(g))))#
		mods_i = paste(evolved_list[[i]][[2]], collapse="; ")#
		mods = c(mods, mods_i)#
		ascii_filename = paste(evolved_graph_log_dir, "ASCII/evolved_graph_",#
							   evolved_graph_ids[i], ".txt", sep="")#
		r_filename = paste(evolved_graph_log_dir, "R_objects/evolved_graph_",#
						   evolved_graph_ids[i], ".R", sep="")#
		save(g, file=r_filename)#
		dput(g, file=ascii_filename)#
	}#
	# todo: add params of evolution process, once those are not hard-coded#
	evolved_graph_log = data.frame("evolved_graph_ids" = evolved_graph_ids,#
								"progenitor_id" = rep(pgid, num_evolved),#
								"sim_run_id" = rep(new_run_ids$run_id, num_evolved),#
								"num_nodes"= num_nodes,#
								"num_edges"= num_edges,#
								"num_mods" = num_mods,#
								"mods"= mods)#
	write.table(evolved_graph_log, evolved_graph_log_file, sep=",", #
				row.names=FALSE, #
				append=file.exists(evolved_graph_log_file), #
				col.names = "!"(file.exists(evolved_graph_log_file)))#
	return(NULL)#
}
log_evolved(evolved_list, new_run_ids, pgid, num_evolved, evolved_graph_log_file)
#################################################################################
# Simulation pipeline:#
#
# load required libraries#
library(graph)#
library(RBGL)#
library(rJava)#
library(pcalg)#
library(stringr)#
# library(gRbase)#
# library(dplyr)#
#
# load configuration file#
setwd("/Users/lizzie/Dissertation_code/Simulation/R-scripts")#
source("simulation_config.R")#
source("graph-rewriting-grammar.R")#
source("logger.R")#
#
#################################################################################
# Load unique ids of last run, if this is not the first run:#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
new_run_ids <- get_new_run_ids(last_id_log_file)#
#
#################################################################################
# file to log every run of the pipeline#
# should contain: run id, date, progenitor graph id, #
#     list of evolved graph ids, list of dataset ids, list of pooled data ids, #
#     list of search ids, list of evaluation set ids, start time, end time,#
#     duration#
sim_run_log = data.frame("run_id"=new_run_ids$run_id, #
						 "start_time"=Sys.time(),#
						 "end_time"=NA,#
						 "duration"=NA,#
						 "progenitor_graph_id"=new_run_ids$progenitor_graph_id,#
						 "evolved_graph_ids"=NA,#
						 "dataset_ids"=NA,#
						 "pooled_data_id"=new_run_ids$pooled_dataset_id,#
						 "target_graph_id"=new_run_ids$evolved_graph_id,#
						 "search_ids"=NA,#
						 "evaluation_ids"=NA)#
#
#################################################################################
# Graph generation:#
# 	Generate small number of random graphs, the progenitors. #
# 		Choices: number of nodes per graph, number of edges per graph (or #
#           connection probability)#
# 		Log: features of graphs, plus a unique id, plus params of generation #
#           process#
# 		Save graph object#
#
progenitor <- randomDAG(num_nodes, prob_edges)#
#
# save and log aspects of the graph in a CSV:#
pgid <- log_progenitor(progenitor, new_run_ids, num_nodes, prob_edges,#
					   progenitor_graph_log_file, progenitor_graph_log_dir)#
#
#################################################################################
# 	Take progenitor and evolve it for a few steps into a descendant. Repeat. #
# 		Choices: number of descendants, number of steps of evolution.#
# 		Log: features of graphs, plus a unique id, plus id of progenitor, plus #
#           params of evolution process, plus specific modifications made#
# 		Save graph object#
# 		At end of repeat: remove progenitor from memory#
#
evolved_list = list()#
for (i in 1:num_evolved){#
	evolved_list[[i]] <- evolveDAG(num_mods, progenitor)#
}#
#
target_graph = evolved_list[[1]][[1]]#
#
log_evolved(evolved_list, new_run_ids, pgid, num_evolved, evolved_graph_log_file)						#
#
sim_run_log$evolved_graph_ids = paste(evolved_graph_ids, collapse=";")#
#
#################################################################################
# 	Generate data from each descendant graph. #
# 		Choices: parameterization, number of data points to generate.#
# 		Log: data filename, number of rows and columns, params of data #
#           generation, id of generator graph#
# 		Save data#
# 		At end of each data generation: remove graph from memory#
#
data_list = list()#
#
dataset_ids = evolved_graph_ids#
#
for (i in 1:num_evolved){#
	if (parameterization=="linear_gaussian"){#
		evolved_list[[i]][[1]] <- dag2gausspardag(evolved_list[[i]][[1]])#
	} else {#
		cat("error: parameterization not listed or not recognized\n")#
		break#
	}#
	data_list[[i]] <- rmvnorm.ivent(sample_size, evolved_list[[i]][[1]])#
	# save dataset#
	data_filename = paste(dataset_log_dir, "dataset_", dataset_ids[i], ".csv", #
						  sep="")#
	write.table(data_list[[i]], data_filename, sep=",", row.names=FALSE, #
				col.names=TRUE)#
}#
# log features of dataset#
# todo: add params of data generation#
# todo: log parameterized graph!!!#
dataset_log = data.frame("dataset_id" = dataset_ids,#
						 "evolved_graph_id"=evolved_graph_ids,#
						 "parameterization" = parameterization,#
						 "num_rows"=rep(sample_size, num_evolved),#
						 "num_columns"= num_nodes)#
#
write.table(dataset_log, dataset_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(dataset_log_file), #
    		col.names = "!"(file.exists(dataset_log_file)))#
#
sim_run_log$dataset_ids = paste(dataset_ids, collapse=";")#
#
# # if not doing evaluation in same loop, get rid of graphs:#
# rm(evolved_list) #
#
# todo: Getting some errors in this step - need to add some unit tests for the graph#
# modification procedure.#
# does duplicating a node cause problems? Maybe if you add an edge from the #
# duplicated node to other nodes? Need to check this.#
#
#################################################################################
# Data pooling:#
#
# 	Pool data from graphs descended from the same progenitor. #
# 		Choices: amount of data from each graph to include; number of graphs #
#           to include#
# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row #
#           numbers from the source data that were used in pooling, #
#
vars = colnames(data_list[[1]])#
rows_included_list = list()#
for (i in 1: num_evolved){#
	vars = intersect(vars, colnames(data_list[[i]]))#
	rows_included = sample(1:sample_size, pooled_sample_sizes[i])#
	rows_included_list[[i]] <- 1:sample_size %in% rows_included#
}#
# save and log the rows included and the vars included #
pool_data_list <- list()#
for (i in 1: num_evolved){#
	pool_data_list[[i]] <- subset(data_list[[i]], #
								  subset=rows_included_list[[i]], select=vars)#
}#
pool_data_frame = do.call(rbind, pool_data_list)#
#
pdid = new_run_ids$pooled_dataset_id#
#
pool_log = data.frame("pooled_dataset_id" = pdid,#
					  "component_dataset_ids" = paste(dataset_ids, collapse=";"),#
					  "pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))#
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")#
dput(rows_included_list, file=pool_filename)#
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(pooled_data_log_file), #
    		col.names = "!"(file.exists(pooled_data_log_file)))#
#
#################################################################################
# Graph search:#
#
# this is the hard part, the part where I want to interface with Tetrad!#
#
# 	Choose one graph from the descendants in each pool to be the "target". #
# 		Choice: which graph? Or repeat for all graphs? #
# 		Log: choice of target#
#
# 	Run search algorithm on the pooled data#
#       Choice: Which Tetrad jar? Which algorithms?#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and #
#           all parameters#
target_full_data = data_list[[1]]#
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])#
#
# start the JVM#
.jinit(path_to_tetrad_jar) #
#
# convert dataframes to tetrad dataset#
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)#
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)#
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)#
#
rgids = seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)#
#
############################
# First search: Pooled data#
# initialize GES#
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)#
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)#
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)#
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)#
# search using GES#
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_pooled, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_pooled, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_pooled, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[1],#
						type="pooled_data",#
						dataset_id=pdid,#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
#
############################
# Second search: Starting point + target data#
rgid = rgid + 1#
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))#
# search using GES#
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_phase_2, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_phase_2, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_phase_2, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[2],#
						type="phase_2",#
						dataset_id=min(dataset_ids),#
						prior_graph=rgid-1,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
############################
# Third search: Target data alone#
rgid = rgid + 1#
ges_solo = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
# search using GES#
tetrad_graph_solo = .jcall(ges_solo, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_solo = tetradPattern2graphNEL(tetrad_graph_solo)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_solo, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_solo, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_solo, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[3],#
						type="target_small_data",#
						dataset_id=min(dataset_ids),#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
eval <- rbind(compareGraphs(ges_graph_solo, target_graph),#
			  compareGraphs(ges_graph_phase_2, target_graph))#
eval <- data.frame(eval)#
eval$eval_ids <- seq(new_run_ids$eval_id, new_run_ids$eval_id + nrow(eval) -1)#
eval$true_graph_id <- rep(min(evolved_graph_ids), nrow(eval))#
eval$recovered_graph_id <- rgids[c(2,3)]#
write.table(eval, eval_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(eval_log_file), #
    		col.names = "!"(file.exists(eval_log_file)))#
#
# todo: log features of graph and parameters of search#
#
#   use resulting graph as (a) a prior, and (b) a starting point for second phase of search#
#   log the prior penalty type, the prior/starting point graph, the data used for the second phase#
#
# 	Run search algorithm on data from individual target graph:#
# 	(1) using just the rows that went into the pooled data#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters#
# 	(2) using a superset, including the pooled data but also additional rows, so that the sample size is equal to the sample size of the pooled data.#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters#
# Evaluation:#
#
# todo: #
# [DONE] 1. set up all logging#
# [DONE] 2. two-stage search #
# 3. prior network search#
# [DONE] 4. evaluate results#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
new_run_ids$dataset_id = max(evolved_graph_ids)#
new_run_ids$search_id = max(rgids)#
new_run_ids$recovered_graph_id = max(rgids)#
new_run_ids$eval_id = max(eval$eval_ids)#
write.table(new_run_ids, last_id_log_file, sep=",", row.names=FALSE, #
    			append=FALSE, col.names = TRUE)#
# log the simulation run#
sim_run_log$end_time <- Sys.time()#
sim_run_log$duration <- sim_run_log$end_time - sim_run_log$start_time#
write.table(sim_run_log, sim_run_log_file, sep=",", row.names=FALSE, #
    append=file.exists(sim_run_log_file), #
    col.names = "!"(file.exists(sim_run_log_file)))
sim_run_log
seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)
#################################################################################
# Simulation pipeline:#
#
# load required libraries#
library(graph)#
library(RBGL)#
library(rJava)#
library(pcalg)#
library(stringr)#
# library(gRbase)#
# library(dplyr)#
#
# load configuration file#
setwd("/Users/lizzie/Dissertation_code/Simulation/R-scripts")#
source("simulation_config.R")#
source("graph-rewriting-grammar.R")#
source("logger.R")#
#
#################################################################################
# Load unique ids of last run, if this is not the first run:#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
new_run_ids <- get_new_run_ids(last_id_log_file)#
#
#################################################################################
# file to log every run of the pipeline#
# should contain: run id, date, progenitor graph id, #
#     list of evolved graph ids, list of dataset ids, list of pooled data ids, #
#     list of search ids, list of evaluation set ids, start time, end time,#
#     duration#
sim_run_log = data.frame("run_id"=new_run_ids$run_id, #
						 "start_time"=Sys.time(),#
						 "end_time"=NA,#
						 "duration"=NA,#
						 "progenitor_graph_id"=new_run_ids$progenitor_graph_id,#
						 "evolved_graph_ids"=NA,#
						 "dataset_ids"=NA,#
						 "pooled_data_id"=new_run_ids$pooled_dataset_id,#
						 "target_graph_id"=new_run_ids$evolved_graph_id,#
						 "search_ids"=NA,#
						 "evaluation_ids"=NA)#
#
#################################################################################
# Graph generation:#
# 	Generate small number of random graphs, the progenitors. #
# 		Choices: number of nodes per graph, number of edges per graph (or #
#           connection probability)#
# 		Log: features of graphs, plus a unique id, plus params of generation #
#           process#
# 		Save graph object#
#
progenitor <- randomDAG(num_nodes, prob_edges)#
#
# save and log aspects of the graph in a CSV:#
pgid <- log_progenitor(progenitor, new_run_ids, num_nodes, prob_edges,#
					   progenitor_graph_log_file, progenitor_graph_log_dir)#
#
#################################################################################
# 	Take progenitor and evolve it for a few steps into a descendant. Repeat. #
# 		Choices: number of descendants, number of steps of evolution.#
# 		Log: features of graphs, plus a unique id, plus id of progenitor, plus #
#           params of evolution process, plus specific modifications made#
# 		Save graph object#
# 		At end of repeat: remove progenitor from memory#
#
evolved_list = list()#
for (i in 1:num_evolved){#
	evolved_list[[i]] <- evolveDAG(num_mods, progenitor)#
}#
#
target_graph = evolved_list[[1]][[1]]#
#
log_evolved(evolved_list, new_run_ids, pgid, num_evolved, evolved_graph_log_file)#
#
sim_run_log$evolved_graph_ids = paste(evolved_graph_ids, collapse=";")#
#
#################################################################################
# 	Generate data from each descendant graph. #
# 		Choices: parameterization, number of data points to generate.#
# 		Log: data filename, number of rows and columns, params of data #
#           generation, id of generator graph#
# 		Save data#
# 		At end of each data generation: remove graph from memory#
#
data_list = list()#
#
dataset_ids = evolved_graph_ids#
#
for (i in 1:num_evolved){#
	if (parameterization=="linear_gaussian"){#
		evolved_list[[i]][[1]] <- dag2gausspardag(evolved_list[[i]][[1]])#
	} else {#
		cat("error: parameterization not listed or not recognized\n")#
		break#
	}#
	data_list[[i]] <- rmvnorm.ivent(sample_size, evolved_list[[i]][[1]])#
	# save dataset#
	data_filename = paste(dataset_log_dir, "dataset_", dataset_ids[i], ".csv", #
						  sep="")#
	write.table(data_list[[i]], data_filename, sep=",", row.names=FALSE, #
				col.names=TRUE)#
}#
#
# log features of dataset#
# todo: add params of data generation#
# todo: log parameterized graph!!!#
dataset_log = data.frame("dataset_id" = dataset_ids,#
						 "evolved_graph_id"=evolved_graph_ids,#
						 "parameterization" = parameterization,#
						 "num_rows"=rep(sample_size, num_evolved),#
						 "num_columns"= num_nodes)#
#
write.table(dataset_log, dataset_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(dataset_log_file), #
    		col.names = "!"(file.exists(dataset_log_file)))#
#
sim_run_log$dataset_ids = paste(dataset_ids, collapse=";")#
#
# # if not doing evaluation in same loop, get rid of graphs:#
# rm(evolved_list) #
#
# todo: Getting some errors in this step - need to add some unit tests for the graph#
# modification procedure.#
# does duplicating a node cause problems? Maybe if you add an edge from the #
# duplicated node to other nodes? Need to check this.#
#
#################################################################################
# Data pooling:#
#
# 	Pool data from graphs descended from the same progenitor. #
# 		Choices: amount of data from each graph to include; number of graphs #
#           to include#
# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row #
#           numbers from the source data that were used in pooling, #
#
vars = colnames(data_list[[1]])#
rows_included_list = list()#
for (i in 1: num_evolved){#
	vars = intersect(vars, colnames(data_list[[i]]))#
	rows_included = sample(1:sample_size, pooled_sample_sizes[i])#
	rows_included_list[[i]] <- 1:sample_size %in% rows_included#
}#
# save and log the rows included and the vars included #
pool_data_list <- list()#
for (i in 1: num_evolved){#
	pool_data_list[[i]] <- subset(data_list[[i]], #
								  subset=rows_included_list[[i]], select=vars)#
}#
pool_data_frame = do.call(rbind, pool_data_list)#
#
pdid = new_run_ids$pooled_dataset_id#
#
pool_log = data.frame("pooled_dataset_id" = pdid,#
					  "component_dataset_ids" = paste(dataset_ids, collapse=";"),#
					  "pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))#
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")#
dput(rows_included_list, file=pool_filename)#
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(pooled_data_log_file), #
    		col.names = "!"(file.exists(pooled_data_log_file)))#
#
#################################################################################
# Graph search:#
#
# this is the hard part, the part where I want to interface with Tetrad!#
#
# 	Choose one graph from the descendants in each pool to be the "target". #
# 		Choice: which graph? Or repeat for all graphs? #
# 		Log: choice of target#
#
# 	Run search algorithm on the pooled data#
#       Choice: Which Tetrad jar? Which algorithms?#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and #
#           all parameters#
target_full_data = data_list[[1]]#
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])#
#
# start the JVM#
.jinit(path_to_tetrad_jar) #
#
# convert dataframes to tetrad dataset#
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)#
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)#
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)#
#
rgids = seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)#
sim_run_log$search_ids <- paste(rgids, collapse=";")#
############################
# First search: Pooled data#
# initialize GES#
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)#
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)#
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)#
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)#
# search using GES#
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[1], ".txt", sep="")#
dput(ges_graph_pooled, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[1], ".R", sep="")#
save(ges_graph_pooled, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[1], ".R", sep="")#
save(tetrad_graph_pooled, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[1],#
						type="pooled_data",#
						dataset_id=pdid,#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
#
############################
# Second search: Starting point + target data#
rgid = rgid + 1#
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))#
# search using GES#
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[2], ".txt", sep="")#
dput(ges_graph_phase_2, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[2], ".R", sep="")#
save(ges_graph_phase_2, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[2], ".R", sep="")#
save(tetrad_graph_phase_2, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[2],#
						type="phase_2",#
						dataset_id=min(dataset_ids),#
						prior_graph=rgid-1,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
############################
# Third search: Target data alone#
rgid = rgid + 1#
ges_solo = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)#
# search using GES#
tetrad_graph_solo = .jcall(ges_solo, "Ledu/cmu/tetrad/graph/Graph;", "search")#
# convert output of GES into an R object (graphNEL)#
ges_graph_solo = tetradPattern2graphNEL(tetrad_graph_solo)#
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_", #
				   rgids[3], ".txt", sep="")#
dput(ges_graph_solo, file=ascii_file)#
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_", #
					  rgids[3], ".R", sep="")#
save(ges_graph_solo, file=r_object_file)#
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",#
						  "recovered_graph_", rgids[3], ".R", sep="")#
save(tetrad_graph_solo, file=rjava_object_file)#
search_log = data.frame(recovered_graph_id = rgids[3],#
						type="target_small_data",#
						dataset_id=min(dataset_ids),#
						prior_graph=NA,#
						path_to_ASCII=ascii_file,#
						path_to_R=r_object_file,#
						path_to_rJava=rjava_object_file)#
write.table(search_log, search_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(search_log_file), #
    		col.names = "!"(file.exists(search_log_file)))#
eval <- rbind(compareGraphs(ges_graph_solo, target_graph),#
			  compareGraphs(ges_graph_phase_2, target_graph))#
eval <- data.frame(eval)#
eval$eval_ids <- seq(new_run_ids$eval_id, new_run_ids$eval_id + nrow(eval) -1)#
eval$true_graph_id <- rep(min(evolved_graph_ids), nrow(eval))#
eval$recovered_graph_id <- rgids[c(2,3)]#
write.table(eval, eval_log_file, sep=",", row.names=FALSE, #
    		append=file.exists(eval_log_file), #
    		col.names = "!"(file.exists(eval_log_file)))#
sim_run_log$evaluation_ids <- paste(eval$eval_ids, collapse=";")#
#
# todo: log features of graph and parameters of search#
#
#   use resulting graph as (a) a prior, and (b) a starting point for second phase of search#
#   log the prior penalty type, the prior/starting point graph, the data used for the second phase#
#
# 	Run search algorithm on data from individual target graph:#
# 	(1) using just the rows that went into the pooled data#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters#
# 	(2) using a superset, including the pooled data but also additional rows, so that the sample size is equal to the sample size of the pooled data.#
# 		Save recovered graph object, and keep in memory for evaluation stage#
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters#
# Evaluation:#
#
# todo: #
# [DONE] 1. set up all logging#
# [DONE] 2. two-stage search #
# 3. prior network search#
# [DONE] 4. evaluate results#
# log the most recent object IDs, so that we always start with a new unique ID #
# for each kind of object#
new_run_ids$evolved_graph_id = max(evolved_graph_ids)#
new_run_ids$dataset_id = max(evolved_graph_ids)#
new_run_ids$search_id = max(rgids)#
new_run_ids$recovered_graph_id = max(rgids)#
new_run_ids$eval_id = max(eval$eval_ids)#
write.table(new_run_ids, last_id_log_file, sep=",", row.names=FALSE, #
    			append=FALSE, col.names = TRUE)#
# log the simulation run#
sim_run_log$end_time <- Sys.time()#
sim_run_log$duration <- sim_run_log$end_time - sim_run_log$start_time#
write.table(sim_run_log, sim_run_log_file, sep=",", row.names=FALSE, #
    append=file.exists(sim_run_log_file), #
    col.names = "!"(file.exists(sim_run_log_file)))
load("/Users/lizzie/Dissertation_code/Simulation/log/recovered_graphs/R_objects/recovered_graph_13.R")
mydata = read.table(path_to_data, sep="\t", col.names=TRUE)
path_to_data = "/Users/lizzie/Dissertation_code/Simulation/R-scripts/chatiry.txt"
mydata = read.table(path_to_data, sep="\t", col.names=TRUE)
path_to_data = "/Users/lizzie/Dissertation_code/Simulation/R-scripts/charity.txt"
mydata = read.table(path_to_data, sep="\t", col.names=TRUE)
mydata = read.table(path_to_data, sep="\t", col.names=TRUE, row.names=FALSE)
mydata = read.table(path_to_data, sep="\t")
mydata
mydata = read.table(path_to_data, sep="\t", header=TRUE)
mydata
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
?graphNEL
#################################################################################
# Interfacing Tetrad with R: a tutorial#
#
# load required libraries#
library(stringr)#
library(graph)#
library(RBGL)#
library(rJava)#
#
# set the paths to whatever they should be for your machine:#
path_to_tetrad_jar = "/Users/lizzie/Dissertation_code/Tetrad-jars/tetrad-5.2.1-3.jar"#
path_to_data = "/Users/lizzie/Dissertation_code/Simulation/R-scripts/charity.txt"#
path_to_utils = "/Users/lizzie/Dissertation_code/Simulation/R-scripts/tetrad_utils.R"#
#
# load the R functions we'll need from this source file#
source(path_to_utils)#
#
# read in some data (I'm using the Charity data for this example)#
mydata = read.table(path_to_data, sep="\t", header=TRUE)#
#
# start the Java Virtual Machine with the Tetrad jar:#
.jinit(path_to_tetrad_jar) #
#
# check you're running the right Java version. This should say 1.8.something:#
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")#
#
# convert dataframe to tetrad dataset:#
# (see function in other R file)#
tetrad_data <- dataFrame2TetradDataset(mydata)#
#
# initialize GES with your data:#
ges_instance = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_data)#
#
# you can now tweak the parameters of GES if you want. For example:#
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)#
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)#
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)#
#
# search using GES:#
tetrad_graph = .jcall(ges_instance, "Ledu/cmu/tetrad/graph/Graph;", "search")#
#
# convert output of GES into an R object (graphNEL)#
ges_graph = tetradPattern2graphNEL(tetrad_graph)#
#
# you can now plot the graph in R:#
plot(ges_graph)#
#
# And you're done!
ids <- c(1,2,3)#
titles <- c("Entry1", "Entry2", "Entry3")#
tags <- c("<self-help><motivation>", "<programming><r><data.frame>", "<photography>")#
df <- data.frame(id = ids, title = titles, tags = tags)#
df
df$tags_list <- lapply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
df
df$tags_list
unlist(df$tags_list)
library(plyr)
df$tags_list <- ldpply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
df$tags_list <- ldply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
df$tags_list <- llply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
df$tags_list <- dlply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
df$tags_list <- llply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
df$tags_list
bob <- llply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
bob <- dlply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
bob <- laply(df$tags, function(x)strsplit(gsub("^.|.$", "", x), "><")[[1]])
strsplit(gsub("^.|.$", "", df$tags), "><")
strsplit(df$tags, "><")
gsub("^.|.$", "", df$tags)
?ldply
