# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row
#           numbers from the source data that were used in pooling,
vars = colnames(data_list[[1]])
rows_included_list = list()
for (i in 1: num_evolved){
vars = intersect(vars, colnames(data_list[[i]]))
rows_included = sample(1:sample_size, pooled_sample_sizes[i])
rows_included_list[[i]] <- 1:sample_size %in% rows_included
}
# save and log the rows included and the vars included
pool_data_list <- list()
for (i in 1: num_evolved){
pool_data_list[[i]] <- subset(data_list[[i]],
subset=rows_included_list[[i]], select=vars)
}
pool_data_frame = do.call(rbind, pool_data_list)
pdid = new_run_ids$pooled_dataset_id
pool_log = data.frame("pooled_dataset_id" = pdid,
"component_dataset_ids" = paste(dataset_ids, collapse=";"),
"pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")
dput(rows_included_list, file=pool_filename)
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE,
append=file.exists(pooled_data_log_file),
col.names = "!"(file.exists(pooled_data_log_file)))
################################################################################
# Graph search:
# this is the hard part, the part where I want to interface with Tetrad!
# 	Choose one graph from the descendants in each pool to be the "target".
# 		Choice: which graph? Or repeat for all graphs?
# 		Log: choice of target
# 	Run search algorithm on the pooled data
#       Choice: Which Tetrad jar? Which algorithms?
# 		Save recovered graph object, and keep in memory for evaluation stage
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and
#           all parameters
target_full_data = data_list[[1]]
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])
# start the JVM
.jinit(path_to_tetrad_jar)
# convert dataframes to tetrad dataset
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)
rgids = seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)
sim_run_log$search_ids <- paste(rgids, collapse=";")
###########################
# First search: Pooled data
# initialize GES
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)
# search using GES
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")
# convert output of GES into an R object (graphNEL)
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_",
rgids[1], ".txt", sep="")
dput(ges_graph_pooled, file=ascii_file)
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_",
rgids[1], ".R", sep="")
save(ges_graph_pooled, file=r_object_file)
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",
"recovered_graph_", rgids[1], ".R", sep="")
save(tetrad_graph_pooled, file=rjava_object_file)
search_log = data.frame(recovered_graph_id = rgids[1],
type="pooled_data",
dataset_id=pdid,
prior_graph=NA,
path_to_ASCII=ascii_file,
path_to_R=r_object_file,
path_to_rJava=rjava_object_file)
write.table(search_log, search_log_file, sep=",", row.names=FALSE,
append=file.exists(search_log_file),
col.names = "!"(file.exists(search_log_file)))
###########################
# Second search: Starting point + target data
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))
# search using GES
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")
# convert output of GES into an R object (graphNEL)
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_",
rgids[2], ".txt", sep="")
dput(ges_graph_phase_2, file=ascii_file)
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_",
rgids[2], ".R", sep="")
save(ges_graph_phase_2, file=r_object_file)
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",
"recovered_graph_", rgids[2], ".R", sep="")
save(tetrad_graph_phase_2, file=rjava_object_file)
search_log = data.frame(recovered_graph_id = rgids[2],
type="phase_2",
dataset_id=min(dataset_ids),
prior_graph=rgids[1],
path_to_ASCII=ascii_file,
path_to_R=r_object_file,
path_to_rJava=rjava_object_file)
write.table(search_log, search_log_file, sep=",", row.names=FALSE,
append=file.exists(search_log_file),
col.names = "!"(file.exists(search_log_file)))
###########################
# Third search: Target data alone
ges_solo = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)
# search using GES
tetrad_graph_solo = .jcall(ges_solo, "Ledu/cmu/tetrad/graph/Graph;", "search")
# convert output of GES into an R object (graphNEL)
ges_graph_solo = tetradPattern2graphNEL(tetrad_graph_solo)
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_",
rgids[3], ".txt", sep="")
dput(ges_graph_solo, file=ascii_file)
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_",
rgids[3], ".R", sep="")
save(ges_graph_solo, file=r_object_file)
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",
"recovered_graph_", rgids[3], ".R", sep="")
save(tetrad_graph_solo, file=rjava_object_file)
search_log = data.frame(recovered_graph_id = rgids[3],
type="target_small_data",
dataset_id=min(dataset_ids),
prior_graph=NA,
path_to_ASCII=ascii_file,
path_to_R=r_object_file,
path_to_rJava=rjava_object_file)
write.table(search_log, search_log_file, sep=",", row.names=FALSE,
append=file.exists(search_log_file),
col.names = "!"(file.exists(search_log_file)))
eval <- rbind(compareGraphOrientations(ges_graph_solo, target_graph),
compareGraphOrientations(ges_graph_phase_2, target_graph))
eval <- data.frame(eval)
eval$eval_ids <- seq(new_run_ids$eval_id, new_run_ids$eval_id + nrow(eval) -1)
eval$true_graph_id <- rep(min(evolved_graph_ids), nrow(eval))
eval$recovered_graph_id <- rgids[c(2,3)]
write.table(eval, eval_log_file, sep=",", row.names=FALSE,
append=file.exists(eval_log_file),
col.names = "!"(file.exists(eval_log_file)))
sim_run_log$evaluation_ids <- paste(eval$eval_ids, collapse=";")
# todo: log features of graph and parameters of search
#   use resulting graph as (a) a prior, and (b) a starting point for second phase of search
#   log the prior penalty type, the prior/starting point graph, the data used for the second phase
# 	Run search algorithm on data from individual target graph:
# 	(1) using just the rows that went into the pooled data
# 		Save recovered graph object, and keep in memory for evaluation stage
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters
# 	(2) using a superset, including the pooled data but also additional rows, so that the
#     sample size is equal to the sample size of the pooled data.
# 		Save recovered graph object, and keep in memory for evaluation stage
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters
# Evaluation:
# todo: prior network search
# todo: evaluate orientations as well as adjacencies
# log the most recent object IDs, so that we always start with a new unique ID
# for each kind of object
new_run_ids$evolved_graph_id = max(evolved_graph_ids)
new_run_ids$dataset_id = max(evolved_graph_ids)
new_run_ids$search_id = max(rgids)
new_run_ids$recovered_graph_id = max(rgids)
new_run_ids$eval_id = max(eval$eval_ids)
write.table(new_run_ids, last_id_log_file, sep=",", row.names=FALSE,
append=FALSE, col.names = TRUE)
# log the simulation run
sim_run_log$end_time <- Sys.time()
sim_run_log$duration <- sim_run_log$end_time - sim_run_log$start_time
write.table(sim_run_log, sim_run_log_file, sep=",", row.names=FALSE,
append=file.exists(sim_run_log_file),
col.names = "!"(file.exists(sim_run_log_file)))
setwd("/Users/lizzie/Dissertation_code/Simulation/log")
sim_runs <- read.csv("simulation_run_log.txt")
searches <- read.csv("search_log.txt")
prog_graphs <- read.csv("progenitor_graph_log.txt")
pooled_datasets <- read.csv("pooled_data_log.txt")
ev_graphs <- read.csv("evolved_graph_log.txt")
evaluation <- read.csv("eval_log.txt")
dataset_table <- read.csv("dataset_log.txt")
View(sim_runs)
View(sim_runs)
View(evaluation)
View(evaluation)
################################################################################
# Simulation pipeline:
# load required libraries
library(graph)
library(RBGL)
library(rJava)
library(pcalg)
library(stringr)
# library(gRbase)
# library(dplyr)
# load configuration file
setwd("/Users/lizzie/Dissertation_code/Simulation/R-scripts")
source("simulation_config.R")
source("graph-rewriting-grammar.R")
source("logger.R")
source("tetrad_utils.R")
################################################################################
# Load unique ids of last run, if this is not the first run:
# log the most recent object IDs, so that we always start with a new unique ID
# for each kind of object
new_run_ids <- get_new_run_ids(last_id_log_file)
################################################################################
# file to log every run of the pipeline
# should contain: run id, date, progenitor graph id,
#     list of evolved graph ids, list of dataset ids, list of pooled data ids,
#     list of search ids, list of evaluation set ids, start time, end time,
#     duration
sim_run_log = data.frame("run_id"=new_run_ids$run_id,
"start_time"=Sys.time(),
"end_time"=NA,
"duration"=NA,
"progenitor_graph_id"=new_run_ids$progenitor_graph_id,
"evolved_graph_ids"=NA,
"dataset_ids"=NA,
"pooled_data_id"=new_run_ids$pooled_dataset_id,
"target_graph_id"=new_run_ids$evolved_graph_id,
"search_ids"=NA,
"evaluation_ids"=NA)
################################################################################
# Graph generation:
# 	Generate small number of random graphs, the progenitors.
# 		Choices: number of nodes per graph, number of edges per graph (or
#           connection probability)
# 		Log: features of graphs, plus a unique id, plus params of generation
#           process
# 		Save graph object
progenitor <- randomDAG(num_nodes, prob_edges)
# save and log aspects of the graph in a CSV:
pgid <- log_progenitor(progenitor, new_run_ids, num_nodes, prob_edges,
progenitor_graph_log_file, progenitor_graph_log_dir)
################################################################################
# 	Take progenitor and evolve it for a few steps into a descendant. Repeat.
# 		Choices: number of descendants, number of steps of evolution.
# 		Log: features of graphs, plus a unique id, plus id of progenitor, plus
#           params of evolution process, plus specific modifications made
# 		Save graph object
# 		At end of repeat: remove progenitor from memory
evolved_list = list()
for (i in 1:num_evolved){
evolved_list[[i]] <- evolveDAG(num_mods, progenitor)
}
target_graph = evolved_list[[1]][[1]]
evolved_graph_ids = log_evolved(evolved_list, new_run_ids, pgid, num_evolved,
evolved_graph_log_file)
sim_run_log$evolved_graph_ids = paste(evolved_graph_ids, collapse=";")
################################################################################
# 	Generate data from each descendant graph.
# 		Choices: parameterization, number of data points to generate.
# 		Log: data filename, number of rows and columns, params of data
#           generation, id of generator graph
# 		Save data
# 		At end of each data generation: remove graph from memory
data_list = list()
dataset_ids = evolved_graph_ids
for (i in 1:num_evolved){
if (parameterization=="linear_gaussian"){
evolved_list[[i]][[1]] <- dag2gausspardag(evolved_list[[i]][[1]])
} else {
cat("error: parameterization not listed or not recognized\n")
break
}
data_list[[i]] <- rmvnorm.ivent(sample_size, evolved_list[[i]][[1]])
# save dataset
data_filename = paste(dataset_log_dir, "dataset_", dataset_ids[i], ".csv",
sep="")
write.table(data_list[[i]], data_filename, sep=",", row.names=FALSE,
col.names=TRUE)
}
# log features of dataset
# todo: add params of data generation
# todo: log parameterized graph!!!
dataset_log = data.frame("dataset_id" = dataset_ids,
"evolved_graph_id"=evolved_graph_ids,
"parameterization" = parameterization,
"num_rows"=rep(sample_size, num_evolved),
"num_columns"= sapply(data_list, "ncol"))
write.table(dataset_log, dataset_log_file, sep=",", row.names=FALSE,
append=file.exists(dataset_log_file),
col.names = "!"(file.exists(dataset_log_file)))
sim_run_log$dataset_ids = paste(dataset_ids, collapse=";")
# # if not doing evaluation in same loop, get rid of graphs:
# rm(evolved_list)
# todo: Getting some errors in this step - need to add some unit tests for the graph
# modification procedure.
# does duplicating a node cause problems? Maybe if you add an edge from the
# duplicated node to other nodes? Need to check this.
################################################################################
# Data pooling:
# 	Pool data from graphs descended from the same progenitor.
# 		Choices: amount of data from each graph to include; number of graphs
#           to include
# 		Log: unique id for pooled dataset, plus the unique dataset IDs and row
#           numbers from the source data that were used in pooling,
vars = colnames(data_list[[1]])
rows_included_list = list()
for (i in 1: num_evolved){
vars = intersect(vars, colnames(data_list[[i]]))
rows_included = sample(1:sample_size, pooled_sample_sizes[i])
rows_included_list[[i]] <- 1:sample_size %in% rows_included
}
# save and log the rows included and the vars included
pool_data_list <- list()
for (i in 1: num_evolved){
pool_data_list[[i]] <- subset(data_list[[i]],
subset=rows_included_list[[i]], select=vars)
}
pool_data_frame = do.call(rbind, pool_data_list)
pdid = new_run_ids$pooled_dataset_id
pool_log = data.frame("pooled_dataset_id" = pdid,
"component_dataset_ids" = paste(dataset_ids, collapse=";"),
"pooled_sample_sizes"=paste(pooled_sample_sizes, collapse=";"))
pool_filename = paste(pooled_data_log_dir, "pooled_rows_", pdid, ".R", sep="")
dput(rows_included_list, file=pool_filename)
write.table(pool_log, pooled_data_log_file, sep=",", row.names=FALSE,
append=file.exists(pooled_data_log_file),
col.names = "!"(file.exists(pooled_data_log_file)))
################################################################################
# Graph search:
# this is the hard part, the part where I want to interface with Tetrad!
# 	Choose one graph from the descendants in each pool to be the "target".
# 		Choice: which graph? Or repeat for all graphs?
# 		Log: choice of target
# 	Run search algorithm on the pooled data
#       Choice: Which Tetrad jar? Which algorithms?
# 		Save recovered graph object, and keep in memory for evaluation stage
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and
#           all parameters
target_full_data = data_list[[1]]
target_small_data = subset(data_list[[1]], subset=rows_included_list[[1]])
# start the JVM
.jinit(path_to_tetrad_jar)
# convert dataframes to tetrad dataset
tetrad_target_full_data <- dataFrame2TetradDataset(target_full_data)
tetrad_target_small_data <- dataFrame2TetradDataset(target_small_data)
tetrad_pooled_data <- dataFrame2TetradDataset(pool_data_frame)
rgids = seq(new_run_ids$recovered_graph_id, new_run_ids$recovered_graph_id + 2)
sim_run_log$search_ids <- paste(rgids, collapse=";")
###########################
# First search: Pooled data
# initialize GES
ges_pooled = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_pooled_data)
#.jcall(gesinstance, "V", "setPenaltyDiscount", 1.0)
#.jcall(gesinstance, "V", "setSamplePrior", 10.0)
#.jcall(gesinstance, "V", "setStructurePrior", 1.0)
# search using GES
tetrad_graph_pooled = .jcall(ges_pooled, "Ledu/cmu/tetrad/graph/Graph;", "search")
# convert output of GES into an R object (graphNEL)
ges_graph_pooled = tetradPattern2graphNEL(tetrad_graph_pooled)
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_",
rgids[1], ".txt", sep="")
dput(ges_graph_pooled, file=ascii_file)
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_",
rgids[1], ".R", sep="")
save(ges_graph_pooled, file=r_object_file)
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",
"recovered_graph_", rgids[1], ".R", sep="")
save(tetrad_graph_pooled, file=rjava_object_file)
search_log = data.frame(recovered_graph_id = rgids[1],
type="pooled_data",
dataset_id=pdid,
prior_graph=NA,
path_to_ASCII=ascii_file,
path_to_R=r_object_file,
path_to_rJava=rjava_object_file)
write.table(search_log, search_log_file, sep=",", row.names=FALSE,
append=file.exists(search_log_file),
col.names = "!"(file.exists(search_log_file)))
###########################
# Second search: Starting point + target data
ges_phase_2 = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)
ges_phase_2$setInitialGraph(.jcast(tetrad_graph_pooled, "edu.cmu.tetrad.graph.Graph"))
# search using GES
tetrad_graph_phase_2 = .jcall(ges_phase_2, "Ledu/cmu/tetrad/graph/Graph;", "search")
# convert output of GES into an R object (graphNEL)
ges_graph_phase_2 = tetradPattern2graphNEL(tetrad_graph_phase_2)
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_",
rgids[2], ".txt", sep="")
dput(ges_graph_phase_2, file=ascii_file)
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_",
rgids[2], ".R", sep="")
save(ges_graph_phase_2, file=r_object_file)
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",
"recovered_graph_", rgids[2], ".R", sep="")
save(tetrad_graph_phase_2, file=rjava_object_file)
search_log = data.frame(recovered_graph_id = rgids[2],
type="phase_2",
dataset_id=min(dataset_ids),
prior_graph=rgids[1],
path_to_ASCII=ascii_file,
path_to_R=r_object_file,
path_to_rJava=rjava_object_file)
write.table(search_log, search_log_file, sep=",", row.names=FALSE,
append=file.exists(search_log_file),
col.names = "!"(file.exists(search_log_file)))
###########################
# Third search: Target data alone
ges_solo = .jnew("edu/cmu/tetrad/search/FastGes", tetrad_target_small_data)
# search using GES
tetrad_graph_solo = .jcall(ges_solo, "Ledu/cmu/tetrad/graph/Graph;", "search")
# convert output of GES into an R object (graphNEL)
ges_graph_solo = tetradPattern2graphNEL(tetrad_graph_solo)
ascii_file = paste(recovered_graph_log_dir, "ASCII/recovered_graph_",
rgids[3], ".txt", sep="")
dput(ges_graph_solo, file=ascii_file)
r_object_file = paste(recovered_graph_log_dir, "R_objects/recovered_graph_",
rgids[3], ".R", sep="")
save(ges_graph_solo, file=r_object_file)
rjava_object_file = paste(recovered_graph_log_dir, "rJava_objects/",
"recovered_graph_", rgids[3], ".R", sep="")
save(tetrad_graph_solo, file=rjava_object_file)
search_log = data.frame(recovered_graph_id = rgids[3],
type="target_small_data",
dataset_id=min(dataset_ids),
prior_graph=NA,
path_to_ASCII=ascii_file,
path_to_R=r_object_file,
path_to_rJava=rjava_object_file)
write.table(search_log, search_log_file, sep=",", row.names=FALSE,
append=file.exists(search_log_file),
col.names = "!"(file.exists(search_log_file)))
eval <- rbind(compareGraphOrientations(ges_graph_solo, target_graph),
compareGraphOrientations(ges_graph_phase_2, target_graph))
eval <- data.frame(eval)
eval$eval_ids <- seq(new_run_ids$eval_id, new_run_ids$eval_id + nrow(eval) -1)
eval$true_graph_id <- rep(min(evolved_graph_ids), nrow(eval))
eval$recovered_graph_id <- rgids[c(2,3)]
write.table(eval, eval_log_file, sep=",", row.names=FALSE,
append=file.exists(eval_log_file),
col.names = "!"(file.exists(eval_log_file)))
sim_run_log$evaluation_ids <- paste(eval$eval_ids, collapse=";")
# todo: log features of graph and parameters of search
#   use resulting graph as (a) a prior, and (b) a starting point for second phase of search
#   log the prior penalty type, the prior/starting point graph, the data used for the second phase
# 	Run search algorithm on data from individual target graph:
# 	(1) using just the rows that went into the pooled data
# 		Save recovered graph object, and keep in memory for evaluation stage
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters
# 	(2) using a superset, including the pooled data but also additional rows, so that the
#     sample size is equal to the sample size of the pooled data.
# 		Save recovered graph object, and keep in memory for evaluation stage
# 		Log: unique id for recovered graph, id of dataset, id of algorithm and all parameters
# Evaluation:
# todo: prior network search
# todo: evaluate orientations as well as adjacencies
# log the most recent object IDs, so that we always start with a new unique ID
# for each kind of object
new_run_ids$evolved_graph_id = max(evolved_graph_ids)
new_run_ids$dataset_id = max(evolved_graph_ids)
new_run_ids$search_id = max(rgids)
new_run_ids$recovered_graph_id = max(rgids)
new_run_ids$eval_id = max(eval$eval_ids)
write.table(new_run_ids, last_id_log_file, sep=",", row.names=FALSE,
append=FALSE, col.names = TRUE)
# log the simulation run
sim_run_log$end_time <- Sys.time()
sim_run_log$duration <- sim_run_log$end_time - sim_run_log$start_time
write.table(sim_run_log, sim_run_log_file, sep=",", row.names=FALSE,
append=file.exists(sim_run_log_file),
col.names = "!"(file.exists(sim_run_log_file)))
setwd("/Users/lizzie/Dissertation_code/Simulation/log")
sim_runs <- read.csv("simulation_run_log.txt")
searches <- read.csv("search_log.txt")
prog_graphs <- read.csv("progenitor_graph_log.txt")
pooled_datasets <- read.csv("pooled_data_log.txt")
ev_graphs <- read.csv("evolved_graph_log.txt")
evaluation <- read.csv("eval_log.txt")
dataset_table <- read.csv("dataset_log.txt")
View(searches)
View(searches)
View(search_log)
anytime.pag
str(anytime.pag)
str(anytime.pag$amat)
str(anytime.pag@amat)
anytime.pag@amat
class(anytime.pag@amat)
anytime.pag@n
plot(anytime.pag)
plot(anytime.pag)
plot.function
fciAlgo-class
class(anytime.pag)
?`fciAlgo-class`
plot.fciAlgo
plot.fciAlgo-class
?plot.fciAlgo
show(anytime.pag)
summary(anytime.pag)
?fci
plot(anytime.pag)
plot(anytime.pag)
??igraph
library(igraph)
?"igraph"
ls()
bob
bob = c("joe", "sam", "ted")
as.character(bob)
.jclass(as.character(bob))
?.jclass
?.jclass()
.jcall(as.character(bob), "Ljava/lang/Class;", "getClass")
joe = .jnew("Ljava/lang/String;", as.character(bob))
?.jnew
joe = .jnew("java.lang.String", as.character(bob))
joe = .jnew("java.lang.String", as.character("bob"))
joe
.jcall(joe, "Ljava/lang/Class;", "getClass")
joe = .jarray(as.character("bob"))
joe
joe = .jarray(as.vector("bob"))
joe
